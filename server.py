from flask import (
    Flask,
    request,
    render_template,
    jsonify,
    send_from_directory
)
import face_recognition
import array as arr
import cv2
import os
from pathlib import Path
import imutils
try:
    from PIL import Image
except ImportError:
    import Image
import uuid
from werkzeug.utils import secure_filename
import base64
import json
import re
from openai import OpenAI
from dotenv import load_dotenv
import numpy as np
from deepface import DeepFace
import warnings
warnings.filterwarnings('ignore')

# Cargar variables de entorno FORZANDO override
load_dotenv(override=True)

# FORZAR lectura directa del archivo .env
with open('.env', 'r') as f:
    for line in f:
        if line.startswith('OPENAI_API_KEY='):
            api_key_loaded = line.split('=', 1)[1].strip()
            break
    else:
        api_key_loaded = None

print(f"ðŸ”‘ API Key del .env: {api_key_loaded[:15] if api_key_loaded else 'NO ENCONTRADA'}...{api_key_loaded[-4:] if api_key_loaded else ''}")

# Inicializar cliente OpenAI
client = OpenAI(api_key=api_key_loaded)

# Create the application instance
app = Flask(__name__, 
           template_folder="templates",
           static_folder="static",
           static_url_path="/static")

# Configuration
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size
UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'mp4', 'avi', 'mov', 'mkv', 'webm'}

# Create upload directory if it doesn't exist
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# Create a URL route in our application for "/"
@app.route('/')
def index():
    return 'API FUNCIONANDO'

# New route to serve the KYC verification flow (original)
@app.route('/test')
def kyc_verification():
    return render_template('kyc_verification.html')

# New professional KYC verification route
@app.route('/kyc')
def kyc_verification_professional():
    return render_template('kyc_verification_new.html')

# Upload endpoint - handles both images and videos
@app.route('/upload', methods=['POST'])
def upload_file():
    try:
        # Check for files in any of these fields
        file = None
        if 'image' in request.files and request.files['image'].filename != '':
            file = request.files['image']
        elif 'video' in request.files and request.files['video'].filename != '':
            file = request.files['video']
        
        if not file:
            return jsonify({'error': 'No file uploaded'}), 400
        
        if file and file.filename != '':
            if allowed_file(file.filename):
                # Generate unique filename
                filename = str(uuid.uuid4()) + '_' + secure_filename(file.filename)
                filepath = os.path.join(UPLOAD_FOLDER, filename)
                file.save(filepath)
                
                # Use forward slashes for URLs
                url_path = filepath.replace('\\', '/')
                
                return jsonify({
                    'success': True,
                    'path': url_path,
                    'filename': filename
                })
            else:
                return jsonify({'error': 'File type not allowed'}), 400
        
        return jsonify({'error': 'No file selected'}), 400
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# Serve uploaded files
@app.route('/uploads/<filename>')
def uploaded_file(filename):
    return send_from_directory(UPLOAD_FOLDER, filename)

# =============================================================================
# ðŸ—ï¸ API MODULAR KYC - ENDPOINTS PARA INTEGRACIÃ“N EN OTRAS APLICACIONES
# =============================================================================

@app.route('/kyc/validate-dni', methods=['POST'])
def validate_dni():
    """
    ðŸ“‹ PASO 1: Validar datos del cliente contra DNI (delante y detrÃ¡s)
    
    Recibe:
    - firstName: Nombre del cliente
    - lastName: Apellidos del cliente  
    - dni: NÃºmero de DNI (ej: 48764016Z)
    - birthDate: Fecha nacimiento (DD/MM/YYYY)
    - address: DirecciÃ³n completa
    - dniImageFront: Foto DNI cara delantera (archivo)
    - dniImageBack: Foto DNI cara trasera (archivo) [OPCIONAL]
    
    Proceso:
    - GPT Vision extrae todos los datos del DNI
    - Compara datos introducidos vs extraÃ­dos
    - Calcula confianza de coincidencia
    
    Devuelve:
    {
        "success": true/false,
        "confidence": 0-100,
        "data_matches": {
            "name": true/false,
            "dni": true/false, 
            "birthdate": true/false,
            "address": true/false
        },
        "extracted_data": {...},
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print("ðŸ“‹ VALIDANDO DNI - Datos del cliente vs documento")
        
        # 1. Recibir datos del formulario
        first_name = request.form.get('firstName', '').strip()
        last_name = request.form.get('lastName', '').strip()
        dni_number = request.form.get('dni', '').strip()
        birth_date = request.form.get('birthDate', '').strip()
        address = request.form.get('address', '').strip()
        
        print(f"ðŸ“‹ Datos recibidos: {first_name} {last_name}, DNI: {dni_number}")
        
        # Validar que se recibieron todos los datos obligatorios
        if not all([first_name, last_name, dni_number, birth_date]):
            return jsonify({
                "success": False,
                "error": "Faltan datos obligatorios: firstName, lastName, dni, birthDate",
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # 2. Recibir y procesar imagen del DNI (cara delantera obligatoria)
        dni_front_file = request.files.get('dniImageFront')
        dni_back_file = request.files.get('dniImageBack')  # Opcional
        
        if not dni_front_file or dni_front_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere imagen del DNI (dniImageFront)",
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # 3. Guardar imagen DNI delantera
        if not allowed_file(dni_front_file.filename):
            return jsonify({
                "success": False,
                "error": "Tipo de archivo no permitido para DNI",
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        dni_front_filename = str(uuid.uuid4()) + '_dni_front_' + secure_filename(dni_front_file.filename)
        dni_front_path = os.path.join(UPLOAD_FOLDER, dni_front_filename)
        dni_front_file.save(dni_front_path)
        
        print(f"ðŸ“‹ DNI guardado en: {dni_front_path}")
        
        # 4. Procesar DNI trasero si se proporciona
        dni_back_path = None
        if dni_back_file and dni_back_file.filename != '':
            if allowed_file(dni_back_file.filename):
                dni_back_filename = str(uuid.uuid4()) + '_dni_back_' + secure_filename(dni_back_file.filename)
                dni_back_path = os.path.join(UPLOAD_FOLDER, dni_back_filename)
                dni_back_file.save(dni_back_path)
                print(f"ðŸ“‹ DNI trasero guardado en: {dni_back_path}")
        
        # 5. Usar GPT Vision para extraer y comparar datos
        user_data = {
            'firstName': first_name,
            'lastName': last_name,
            'dni': dni_number,
            'birthDate': birth_date,
            'address': address
        }
        
        # AnÃ¡lisis con GPT Vision de la cara delantera
        print("ðŸ§  Analizando DNI con GPT Vision...")
        gpt_result = analyze_and_compare_dni_with_gpt(dni_front_path, user_data)
        
        # Intentar parsear resultado JSON de GPT
        try:
            if isinstance(gpt_result, str):
                result_data = json.loads(gpt_result)
            else:
                result_data = gpt_result
                
            # Estructura de respuesta del endpoint
            response = {
                "success": True,
                "confidence": result_data.get('verification', {}).get('overall_confidence', 0),
                "data_matches": {
                    "name": result_data.get('verification', {}).get('name_match', False),
                    "dni": result_data.get('verification', {}).get('dni_match', False),
                    "birthdate": result_data.get('verification', {}).get('birthdate_match', False),
                    "address": result_data.get('verification', {}).get('address_match', False)
                },
                "extracted_data": result_data.get('extracted_data', {}),
                "extracted_text": result_data.get('extracted_text', ''),
                "details": result_data.get('details', ''),
                "recommendation": result_data.get('verification', {}).get('recommendation', 'REVIEW'),
                "dni_front_path": dni_front_path.replace('\\', '/'),
                "dni_back_path": dni_back_path.replace('\\', '/') if dni_back_path else None
            }
            
            print(f"âœ… ValidaciÃ³n completada - Confianza: {response['confidence']}%")
            return jsonify(response)
            
        except (json.JSONDecodeError, KeyError) as e:
            print(f"âš ï¸ Error parseando respuesta GPT: {e}")
            # Respuesta de fallback si GPT no devuelve JSON vÃ¡lido
            return jsonify({
                "success": False,
                "error": "Error procesando respuesta de anÃ¡lisis",
                "confidence": 0,
                "recommendation": "REVIEW",
                "raw_response": str(gpt_result)
            })
        
    except Exception as e:
        print(f"âŒ Error validando DNI: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/kyc/verify-selfie', methods=['POST'])
def verify_selfie():
    """
    ðŸ¤³ PASO 2: Verificar que el selfie coincide con la persona del DNI
    
    Recibe:
    - dniImagePath: Ruta de la imagen DNI ya procesada (del paso anterior)
    - selfieImage: Foto selfie del cliente (archivo)
    
    Proceso:
    - GPT Vision compara la cara del DNI con el selfie
    - Analiza similitudes faciales, edad, caracterÃ­sticas
    - Detecta posibles intentos de fraude
    
    Devuelve:
    {
        "success": true/false,
        "face_match": true/false,
        "confidence": 0-100,
        "analysis": "DescripciÃ³n detallada de la comparaciÃ³n",
        "fraud_indicators": [...],
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print("ðŸ¤³ VERIFICANDO SELFIE - ComparaciÃ³n con DNI")
        
        # 1. Recibir ruta de imagen DNI ya procesada
        dni_image_path = request.form.get('dniImagePath', '').strip()
        
        # 2. Recibir archivo de selfie
        selfie_file = request.files.get('selfieImage')
        
        # Validaciones
        if not dni_image_path:
            return jsonify({
                "success": False,
                "error": "Se requiere la ruta de la imagen DNI (dniImagePath)",
                "face_match": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        if not selfie_file or selfie_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere imagen del selfie (selfieImage)",
                "face_match": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # Verificar que el archivo DNI existe
        if not os.path.exists(dni_image_path):
            return jsonify({
                "success": False,
                "error": f"Imagen DNI no encontrada: {dni_image_path}",
                "face_match": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # 3. Guardar imagen del selfie
        if not allowed_file(selfie_file.filename):
            return jsonify({
                "success": False,
                "error": "Tipo de archivo no permitido para selfie",
                "face_match": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        selfie_filename = str(uuid.uuid4()) + '_selfie_' + secure_filename(selfie_file.filename)
        selfie_path = os.path.join(UPLOAD_FOLDER, selfie_filename)
        selfie_file.save(selfie_path)
        
        print(f"ðŸ¤³ Selfie guardado en: {selfie_path}")
        print(f"ðŸ¤³ Comparando con DNI: {dni_image_path}")
        
        # 4. Usar face_recognition para comparar caras DNI vs Selfie
        print("ðŸ§  Comparando caras con face_recognition...")
        comparison_result = compare_faces_with_face_recognition(dni_image_path, selfie_path)
        
        # 5. Estructurar respuesta del endpoint
        response = {
            "success": True,
            "face_match": comparison_result.get('face_match', False),
            "confidence": comparison_result.get('confidence', 0),
            "analysis": comparison_result.get('analysis', 'Sin anÃ¡lisis disponible'),
            "fraud_indicators": comparison_result.get('fraud_indicators', []),
            "recommendation": comparison_result.get('recommendation', 'REVIEW'),
            "selfie_path": selfie_path.replace('\\', '/'),
            "dni_path": dni_image_path.replace('\\', '/'),
            "gpt_raw_response": comparison_result
        }
        
        print(f"âœ… VerificaciÃ³n selfie completada - Match: {response['face_match']}, Confianza: {response['confidence']}%")
        return jsonify(response)
        
    except Exception as e:
        print(f"âŒ Error verificando selfie: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/kyc/verify-liveness', methods=['POST'])
def verify_liveness():
    """
    ðŸŽ¥ PASO 3: Verificar que el video es de una persona real y coincide
    
    Recibe:
    - selfieImagePath: Ruta del selfie ya verificado (del paso anterior)
    - dniImagePath: Ruta de la imagen DNI (referencia original)
    - verificationVideo: Video de verificaciÃ³n en vivo (archivo)
    
    Proceso:
    - GPT Vision analiza frames del video
    - Verifica que es una persona real (no foto/pantalla)
    - Compara con selfie y DNI para triple verificaciÃ³n
    - Detecta movimientos naturales y caracterÃ­sticas de vida
    
    Devuelve:
    {
        "success": true/false,
        "is_live_person": true/false,
        "matches_selfie": true/false,
        "matches_dni": true/false,
        "confidence": 0-100,
        "liveness_score": 0-100,
        "analysis": "AnÃ¡lisis detallado del video",
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print("ðŸŽ¥ VERIFICANDO VIDA REAL - AnÃ¡lisis de video")
        
        # 1. Recibir rutas de imÃ¡genes de referencia
        selfie_image_path = request.form.get('selfieImagePath', '').strip()
        dni_image_path = request.form.get('dniImagePath', '').strip()
        
        # 2. Recibir archivo de video
        video_file = request.files.get('verificationVideo')
        
        # Validaciones
        if not selfie_image_path:
            return jsonify({
                "success": False,
                "error": "Se requiere la ruta de la imagen selfie (selfieImagePath)",
                "is_live_person": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        if not video_file or video_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere el video de verificaciÃ³n (verificationVideo)",
                "is_live_person": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # Verificar que las imÃ¡genes de referencia existen
        if not os.path.exists(selfie_image_path):
            return jsonify({
                "success": False,
                "error": f"Imagen selfie no encontrada: {selfie_image_path}",
                "is_live_person": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        if dni_image_path and not os.path.exists(dni_image_path):
            print(f"âš ï¸ DNI no encontrado: {dni_image_path}, solo usando selfie como referencia")
            dni_image_path = None
        
        # 3. Guardar video de verificaciÃ³n
        if not allowed_file(video_file.filename):
            return jsonify({
                "success": False,
                "error": "Tipo de archivo no permitido para video",
                "is_live_person": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        video_filename = str(uuid.uuid4()) + '_liveness_' + secure_filename(video_file.filename)
        video_path = os.path.join(UPLOAD_FOLDER, video_filename)
        video_file.save(video_path)
        
        print(f"ðŸŽ¥ Video guardado en: {video_path}")
        print(f"ðŸŽ¥ Comparando con selfie: {selfie_image_path}")
        if dni_image_path:
            print(f"ðŸŽ¥ TambiÃ©n comparando con DNI: {dni_image_path}")
        
        # 4. Analizar video con GPT Vision
        print("ðŸ§  Analizando liveness con GPT Vision...")
        
        # AnÃ¡lisis principal: video vs selfie
        liveness_result = gpt_vision_analyze_video_frames(video_path, selfie_image_path)
        
        # AnÃ¡lisis adicional: video vs DNI (si estÃ¡ disponible)
        dni_comparison_result = None
        if dni_image_path:
            print("ðŸ§  ComparaciÃ³n adicional video vs DNI...")
            dni_comparison_result = gpt_vision_analyze_video_frames(video_path, dni_image_path)
        
        # 5. Combinar resultados si hay comparaciÃ³n con DNI
        final_confidence = liveness_result.get('confidence', 0)
        final_liveness_score = liveness_result.get('liveness_score', 0)
        matches_selfie = liveness_result.get('matches_reference', False)
        matches_dni = False
        
        if dni_comparison_result:
            matches_dni = dni_comparison_result.get('matches_reference', False)
            dni_confidence = dni_comparison_result.get('confidence', 0)
            
            # Promedio ponderado: selfie 70%, DNI 30%
            final_confidence = int((final_confidence * 0.7) + (dni_confidence * 0.3))
            
            print(f"ðŸŽ¥ Confianza combinada: selfie={liveness_result.get('confidence')}%, dni={dni_confidence}%, final={final_confidence}%")
        
        # 6. Determinar recomendaciÃ³n final
        is_live_person = liveness_result.get('is_live_person', False)
        
        if is_live_person and matches_selfie and final_liveness_score >= 85:
            if dni_comparison_result is None or matches_dni or final_confidence >= 80:
                recommendation = "APPROVE"
            else:
                recommendation = "REVIEW"  # Live y match selfie, pero no match DNI
        elif is_live_person and matches_selfie and final_liveness_score >= 65:
            recommendation = "REVIEW"
        else:
            recommendation = "REJECT"
        
        # 7. Compilar anÃ¡lisis detallado
        analysis_parts = [
            liveness_result.get('analysis', 'Sin anÃ¡lisis disponible'),
        ]
        
        if dni_comparison_result:
            analysis_parts.append(f"ComparaciÃ³n adicional con DNI: {'Coincide' if matches_dni else 'No coincide'}")
        
        final_analysis = ". ".join(analysis_parts)
        
        # 8. Estructurar respuesta del endpoint
        response = {
            "success": True,
            "is_live_person": is_live_person,
            "matches_selfie": matches_selfie,
            "matches_dni": matches_dni,
            "confidence": final_confidence,
            "liveness_score": final_liveness_score,
            "analysis": final_analysis,
            "recommendation": recommendation,
            "video_path": video_path.replace('\\', '/'),
            "frames_analyzed": liveness_result.get('frames_analyzed', 0),
            "consistency_scores": liveness_result.get('consistency_scores', []),
            "selfie_comparison": liveness_result,
            "dni_comparison": dni_comparison_result
        }
        
        print(f"âœ… VerificaciÃ³n liveness completada - Live: {is_live_person}, Selfie: {matches_selfie}, DNI: {matches_dni}, Score: {final_liveness_score}%")
        return jsonify(response)
        
    except Exception as e:
        print(f"âŒ Error verificando vida real: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/kyc/complete-verification', methods=['POST'])
def complete_verification():
    """
    âœ… ENDPOINT COMPLETO: VerificaciÃ³n KYC total en una sola llamada
    
    Recibe:
    - firstName, lastName, dni, birthDate, address (datos del cliente)
    - dniImageFront: Foto DNI cara delantera
    - dniImageBack: Foto DNI cara trasera [OPCIONAL]
    - selfieImage: Foto selfie del cliente
    - verificationVideo: Video de verificaciÃ³n en vivo
    
    Proceso:
    - Ejecuta automÃ¡ticamente los 3 pasos anteriores
    - Combina todos los resultados
    - Genera veredicto final con mÃ¡xima confianza
    
    Devuelve:
    {
        "success": true/false,
        "final_recommendation": "APPROVE/REVIEW/REJECT",
        "overall_confidence": 0-100,
        "verification_steps": {
            "dni_validation": {...},
            "selfie_verification": {...},
            "liveness_verification": {...}
        },
        "summary": "Resumen ejecutivo de la verificaciÃ³n",
        "timestamp": "2024-01-01T00:00:00Z"
    }
    """
    try:
        print("âœ… VERIFICACIÃ“N COMPLETA KYC - Proceso automÃ¡tico total")
        
        # 1. Recibir todos los datos del formulario
        first_name = request.form.get('firstName', '').strip()
        last_name = request.form.get('lastName', '').strip()
        dni_number = request.form.get('dni', '').strip()
        birth_date = request.form.get('birthDate', '').strip()
        address = request.form.get('address', '').strip()
        
        print(f"âœ… Cliente: {first_name} {last_name}, DNI: {dni_number}")
        
        # Validar datos obligatorios
        if not all([first_name, last_name, dni_number, birth_date]):
            return jsonify({
                "success": False,
                "error": "Faltan datos obligatorios del cliente",
                "final_recommendation": "REJECT",
                "overall_confidence": 0
            }), 400
        
        # 2. Recibir todos los archivos
        dni_front_file = request.files.get('dniImageFront')
        dni_back_file = request.files.get('dniImageBack')  # Opcional
        selfie_file = request.files.get('selfieImage')
        video_file = request.files.get('verificationVideo')
        
        # Validar archivos obligatorios
        if not dni_front_file or dni_front_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere imagen del DNI frontal",
                "final_recommendation": "REJECT",
                "overall_confidence": 0
            }), 400
        
        if not selfie_file or selfie_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere imagen selfie",
                "final_recommendation": "REJECT",
                "overall_confidence": 0
            }), 400
        
        if not video_file or video_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere video de verificaciÃ³n",
                "final_recommendation": "REJECT",
                "overall_confidence": 0
            }), 400
        
        # 3. PASO 1: ValidaciÃ³n de DNI
        print("ðŸ“‹ PASO 1: Validando DNI...")
        
        # Guardar DNI frontal
        dni_front_filename = str(uuid.uuid4()) + '_complete_dni_front_' + secure_filename(dni_front_file.filename)
        dni_front_path = os.path.join(UPLOAD_FOLDER, dni_front_filename)
        dni_front_file.save(dni_front_path)
        
        # Guardar DNI trasero si existe
        dni_back_path = None
        if dni_back_file and dni_back_file.filename != '':
            dni_back_filename = str(uuid.uuid4()) + '_complete_dni_back_' + secure_filename(dni_back_file.filename)
            dni_back_path = os.path.join(UPLOAD_FOLDER, dni_back_filename)
            dni_back_file.save(dni_back_path)
        
        # Validar DNI con GPT Vision
        user_data = {
            'firstName': first_name,
            'lastName': last_name,
            'dni': dni_number,
            'birthDate': birth_date,
            'address': address
        }
        
        gpt_dni_result = analyze_and_compare_dni_with_gpt(dni_front_path, user_data)
        
        # Parsear resultado del DNI
        try:
            if isinstance(gpt_dni_result, str):
                dni_data = json.loads(gpt_dni_result)
            else:
                dni_data = gpt_dni_result
                
            dni_verification = {
                "success": True,
                "confidence": dni_data.get('verification', {}).get('overall_confidence', 0),
                "recommendation": dni_data.get('verification', {}).get('recommendation', 'REVIEW'),
                "data_matches": {
                    "name": dni_data.get('verification', {}).get('name_match', False),
                    "dni": dni_data.get('verification', {}).get('dni_match', False),
                    "birthdate": dni_data.get('verification', {}).get('birthdate_match', False),
                    "address": dni_data.get('verification', {}).get('address_match', False)
                },
                "extracted_data": dni_data.get('extracted_data', {}),
                "details": dni_data.get('details', '')
            }
        except:
            dni_verification = {
                "success": False,
                "confidence": 0,
                "recommendation": "REJECT",
                "error": "Error procesando DNI"
            }
        
        print(f"ðŸ“‹ DNI ValidaciÃ³n: {dni_verification['recommendation']} ({dni_verification['confidence']}%)")
        
        # 4. PASO 2: VerificaciÃ³n de Selfie
        print("ðŸ¤³ PASO 2: Verificando selfie...")
        
        # Guardar selfie
        selfie_filename = str(uuid.uuid4()) + '_complete_selfie_' + secure_filename(selfie_file.filename)
        selfie_path = os.path.join(UPLOAD_FOLDER, selfie_filename)
        selfie_file.save(selfie_path)
        
        # Comparar DNI vs Selfie con GPT Vision
        selfie_comparison = gpt_vision_compare_faces(
            dni_front_path, 
            selfie_path, 
            comparison_type="dni_vs_selfie"
        )
        
        selfie_verification = {
            "success": True,
            "face_match": selfie_comparison.get('face_match', False),
            "confidence": selfie_comparison.get('confidence', 0),
            "recommendation": selfie_comparison.get('recommendation', 'REVIEW'),
            "analysis": selfie_comparison.get('analysis', ''),
            "fraud_indicators": selfie_comparison.get('fraud_indicators', [])
        }
        
        print(f"ðŸ¤³ Selfie VerificaciÃ³n: {selfie_verification['recommendation']} ({selfie_verification['confidence']}%)")
        
        # 5. PASO 3: VerificaciÃ³n de Liveness
        print("ðŸŽ¥ PASO 3: Verificando liveness...")
        
        # Guardar video
        video_filename = str(uuid.uuid4()) + '_complete_video_' + secure_filename(video_file.filename)
        video_path = os.path.join(UPLOAD_FOLDER, video_filename)
        video_file.save(video_path)
        
        # Analizar liveness con video vs selfie (y opcionalmente DNI)
        liveness_result = gpt_vision_analyze_video_frames(video_path, selfie_path)
        
        # AnÃ¡lisis adicional con DNI si estÃ¡ disponible
        dni_liveness_result = None
        if dni_front_path:
            dni_liveness_result = gpt_vision_analyze_video_frames(video_path, dni_front_path)
        
        # Combinar resultados de liveness
        liveness_confidence = liveness_result.get('confidence', 0)
        if dni_liveness_result:
            dni_liveness_confidence = dni_liveness_result.get('confidence', 0)
            liveness_confidence = int((liveness_confidence * 0.7) + (dni_liveness_confidence * 0.3))
        
        liveness_verification = {
            "success": True,
            "is_live_person": liveness_result.get('is_live_person', False),
            "matches_selfie": liveness_result.get('matches_reference', False),
            "matches_dni": dni_liveness_result.get('matches_reference', False) if dni_liveness_result else None,
            "confidence": liveness_confidence,
            "liveness_score": liveness_result.get('liveness_score', 0),
            "recommendation": liveness_result.get('recommendation', 'REVIEW'),
            "analysis": liveness_result.get('analysis', ''),
            "frames_analyzed": liveness_result.get('frames_analyzed', 0)
        }
        
        print(f"ðŸŽ¥ Liveness VerificaciÃ³n: {liveness_verification['recommendation']} ({liveness_verification['confidence']}%)")
        
        # 6. CALCULAR VEREDICTO FINAL
        print("ðŸŽ¯ Calculando veredicto final...")
        
        # Pesos para cada verificaciÃ³n
        dni_weight = 0.4      # 40% - Datos del documento
        selfie_weight = 0.35  # 35% - VerificaciÃ³n facial
        liveness_weight = 0.25 # 25% - VerificaciÃ³n de vida
        
        # Calcular confianza general ponderada
        overall_confidence = int(
            (dni_verification['confidence'] * dni_weight) +
            (selfie_verification['confidence'] * selfie_weight) +
            (liveness_verification['confidence'] * liveness_weight)
        )
        
        # Determinar recomendaciÃ³n final
        dni_ok = dni_verification['recommendation'] in ['APPROVE', 'REVIEW']
        selfie_ok = selfie_verification['recommendation'] in ['APPROVE', 'REVIEW'] and selfie_verification['face_match']
        liveness_ok = liveness_verification['recommendation'] in ['APPROVE', 'REVIEW'] and liveness_verification['is_live_person']
        
        # LÃ³gica de decisiÃ³n final
        if all([dni_ok, selfie_ok, liveness_ok]) and overall_confidence >= 85:
            final_recommendation = "APPROVE"
            success = True
        elif all([dni_ok, selfie_ok, liveness_ok]) and overall_confidence >= 70:
            final_recommendation = "REVIEW"
            success = True
        else:
            final_recommendation = "REJECT"
            success = False
        
        # 7. Crear resumen ejecutivo
        issues = []
        if not dni_ok:
            issues.append("Datos del DNI no coinciden")
        if not selfie_ok:
            issues.append("Selfie no coincide con DNI")
        if not liveness_ok:
            issues.append("Video no pasa verificaciÃ³n de vida")
        
        if not issues:
            summary = f"Cliente verificado exitosamente. Confianza general: {overall_confidence}%. Todos los pasos completados satisfactoriamente."
        else:
            summary = f"VerificaciÃ³n {'parcial' if success else 'fallida'}. Problemas detectados: {', '.join(issues)}. Confianza: {overall_confidence}%."
        
        # 8. Timestamp
        from datetime import datetime
        timestamp = datetime.now().isoformat() + "Z"
        
        # 9. Respuesta final completa
        response = {
            "success": success,
            "final_recommendation": final_recommendation,
            "overall_confidence": overall_confidence,
            "verification_steps": {
                "dni_validation": dni_verification,
                "selfie_verification": selfie_verification,
                "liveness_verification": liveness_verification
            },
            "summary": summary,
            "timestamp": timestamp,
            "client_data": {
                "name": f"{first_name} {last_name}",
                "dni": dni_number,
                "birthDate": birth_date,
                "address": address
            },
            "files_processed": {
                "dni_front": dni_front_path.replace('\\', '/'),
                "dni_back": dni_back_path.replace('\\', '/') if dni_back_path else None,
                "selfie": selfie_path.replace('\\', '/'),
                "video": video_path.replace('\\', '/')
            }
        }
        
        print(f"ðŸŽ¯ VEREDICTO FINAL: {final_recommendation} ({overall_confidence}% confianza)")
        print(f"ðŸ“‹ Resumen: {summary}")
        
        return jsonify(response)
        
    except Exception as e:
        print(f"âŒ Error en verificaciÃ³n completa: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

# =============================================================================
# ðŸ§  FUNCIONES DE ANÃLISIS FACIAL
# =============================================================================

def compare_faces_with_face_recognition(image1_path, image2_path):
    """
    ðŸ” ComparaciÃ³n facial avanzada usando DeepFace con mÃºltiples detectores
    
    Sistema hÃ­brido que usa varios algoritmos de detecciÃ³n para maximizar
    la detecciÃ³n de caras en DNIs de baja resoluciÃ³n y selfies.
    
    CaracterÃ­sticas:
    - MÃºltiples detectores: RetinaFace, MTCNN, OpenCV, MediaPipe
    - Modelos de reconocimiento: VGG-Face, ArcFace, Facenet
    - AnÃ¡lisis de calidad de imagen
    - Sistema de confianza inteligente
    - ValidaciÃ³n anti-spoofing
    
    Args:
        image1_path (str): Ruta de la imagen del DNI
        image2_path (str): Ruta del selfie
        
    Returns:
        dict: Resultado completo con anÃ¡lisis, confianza y recomendaciÃ³n
    """
    try:
        print(f"ðŸ” Iniciando anÃ¡lisis facial avanzado con DeepFace")
        print(f"ðŸ“„ DNI: {image1_path}")
        print(f"ðŸ¤³ Selfie: {image2_path}")
        
        # 1. ANÃLISIS DE CALIDAD DE IMÃGENES
        quality_dni = analyze_image_quality_enhanced(image1_path)
        quality_selfie = analyze_image_quality_enhanced(image2_path)
        
        print(f"ðŸ“Š Calidad - DNI: {quality_dni['overall_score']}/100, Selfie: {quality_selfie['overall_score']}/100")
        
        # 2. DETECCIÃ“N MÃšLTIPLE DE CARAS (sistema de fallback)
        detectors = ['retinaface', 'mtcnn', 'opencv', 'mediapipe']
        dni_detection = None
        selfie_detection = None
        
        # Intentar detectar cara en DNI con mÃºltiples detectores
        for detector in detectors:
            try:
                print(f"ðŸ” Probando detector {detector} para DNI...")
                faces_dni = DeepFace.extract_faces(
                    img_path=image1_path,
                    detector_backend=detector,
                    enforce_detection=False,
                    align=True,
                    expand_percentage=10
                )
                if len(faces_dni) > 0 and faces_dni[0]['face'] is not None:
                    dni_detection = {'detector': detector, 'faces': faces_dni}
                    print(f"âœ… Cara detectada en DNI usando {detector}")
                    break
            except Exception as e:
                print(f"âš ï¸ Detector {detector} fallÃ³ para DNI: {str(e)}")
                continue
        
        # Intentar detectar cara en Selfie
        for detector in detectors:
            try:
                print(f"ðŸ” Probando detector {detector} para Selfie...")
                faces_selfie = DeepFace.extract_faces(
                    img_path=image2_path,
                    detector_backend=detector,
                    enforce_detection=False,
                    align=True,
                    expand_percentage=5
                )
                if len(faces_selfie) > 0 and faces_selfie[0]['face'] is not None:
                    selfie_detection = {'detector': detector, 'faces': faces_selfie}
                    print(f"âœ… Cara detectada en Selfie usando {detector}")
                    break
            except Exception as e:
                print(f"âš ï¸ Detector {detector} fallÃ³ para Selfie: {str(e)}")
                continue
        
        # 3. VALIDACIONES DE DETECCIÃ“N
        if dni_detection is None:
            if quality_dni['overall_score'] >= 50:
                return {
                    "face_match": False,
                    "confidence": 0,
                    "analysis": "DNI de calidad aceptable pero no se pudo detectar ninguna cara con ningÃºn algoritmo. Puede ser un documento especial o tener caracterÃ­sticas no estÃ¡ndar.",
                    "quality_check": {
                        "dni_quality": quality_dni,
                        "selfie_quality": quality_selfie
                    },
                    "fraud_indicators": ["Cara no detectable en documento con mÃºltiples algoritmos"],
                    "recommendation": "REVIEW"
                }
            else:
                return {
                    "face_match": False,
                    "confidence": 0,
                    "analysis": "Imagen del DNI de baja calidad. No se puede detectar cara debido a resoluciÃ³n insuficiente, desenfoque o mala iluminaciÃ³n.",
                    "quality_check": {
                        "dni_quality": quality_dni,
                        "selfie_quality": quality_selfie
                    },
                    "fraud_indicators": ["DNI de calidad insuficiente para anÃ¡lisis"],
                    "recommendation": "REJECT"
                }
        
        if selfie_detection is None:
            return {
                "face_match": False,
                "confidence": 0,
                "analysis": "No se pudo detectar ninguna cara en el selfie con ningÃºn algoritmo de detecciÃ³n.",
                "quality_check": {
                    "dni_quality": quality_dni,
                    "selfie_quality": quality_selfie
                },
                "fraud_indicators": ["Sin cara detectable en selfie"],
                "recommendation": "REJECT"
            }
        
        # 4. VERIFICACIÃ“N MÃšLTIPLE CON DIFERENTES MODELOS
        models = ['VGG-Face', 'ArcFace', 'Facenet']
        verification_results = []
        
        best_result = None
        highest_confidence = 0
        
        for model in models:
            try:
                print(f"ðŸ§  Verificando con modelo {model}...")
                
                # THRESHOLDS AJUSTADOS PARA DNI vs SELFIE (mÃ¡s permisivos)
                custom_thresholds = {
                    'VGG-Face': 1.0,    # Default: 0.68, Ajustado: 1.0
                    'ArcFace': 1.0,     # Default: 0.68, Ajustado: 1.0  
                    'Facenet': 0.6      # Default: 0.40, Ajustado: 0.6
                }
                
                # Usar el mejor detector encontrado para cada imagen
                result = DeepFace.verify(
                    img1_path=image1_path,
                    img2_path=image2_path,
                    model_name=model,
                    detector_backend=dni_detection['detector'],  # Usar mejor detector
                    distance_metric='cosine',
                    enforce_detection=False,
                    align=True,
                    expand_percentage=10,
                    threshold=custom_thresholds.get(model, 0.68)  # Usar threshold personalizado
                )
                
                # Calcular confianza personalizada
                confidence_score = calculate_deepface_confidence(result, quality_dni, quality_selfie)
                
                verification_results.append({
                    'model': model,
                    'verified': result['verified'],
                    'distance': result['distance'],
                    'threshold': result['threshold'],
                    'confidence': confidence_score,
                    'detector_used': dni_detection['detector']
                })
                
                if confidence_score > highest_confidence:
                    highest_confidence = confidence_score
                    best_result = result
                    
                print(f"âœ… Modelo {model}: Match={result['verified']}, Distancia={result['distance']:.4f}, Threshold={result['threshold']:.4f}, Confianza={confidence_score:.1f}%")
                
                # DEBUG: Mostrar detalles de por quÃ© falla o pasa
                if result['verified']:
                    print(f"   âœ… {model} CONFIRMA: distancia {result['distance']:.4f} < threshold {result['threshold']:.4f}")
                else:
                    print(f"   âŒ {model} RECHAZA: distancia {result['distance']:.4f} > threshold {result['threshold']:.4f}")
                    print(f"   ðŸ“Š Diferencia: {(result['distance'] - result['threshold']):.4f}")
                
            except Exception as e:
                print(f"âš ï¸ Modelo {model} fallÃ³: {str(e)}")
                continue
        
        if not verification_results:
            return {
                "face_match": False,
                "confidence": 0,
                "analysis": "Error: No se pudo completar la verificaciÃ³n con ningÃºn modelo de reconocimiento facial.",
                "quality_check": {
                    "dni_quality": quality_dni,
                    "selfie_quality": quality_selfie
                },
                "fraud_indicators": ["Fallo tÃ©cnico en todos los modelos"],
                "recommendation": "REJECT"
            }
        
        # 5. ANÃLISIS CONSENSO (mayorÃ­a de modelos)
        verified_count = sum(1 for r in verification_results if r['verified'])
        total_models = len(verification_results)
        consensus_match = verified_count > (total_models / 2)
        
        print(f"ðŸ“Š ANÃLISIS CONSENSO:")
        print(f"   Modelos que confirman: {verified_count}/{total_models}")
        print(f"   Consenso: {consensus_match}")
        
        # 6. CÃLCULO DE CONFIANZA FINAL
        avg_confidence = sum(r['confidence'] for r in verification_results) / len(verification_results)
        final_confidence = max(highest_confidence, avg_confidence)
        
        print(f"ðŸ“Š CONFIANZA FINAL:")
        print(f"   Promedio: {avg_confidence:.1f}%")
        print(f"   MÃ¡xima: {highest_confidence:.1f}%")
        print(f"   Final: {final_confidence:.1f}%")
        
        # 7. ANÃLISIS DE CALIDAD Y DETECCIÃ“N
        detection_analysis = analyze_detection_quality(dni_detection, selfie_detection, quality_dni, quality_selfie)
        
        # 8. DETERMINAR RECOMENDACIÃ“N FINAL
        recommendation, analysis = determine_enhanced_recommendation(
            consensus_match, final_confidence, verification_results, 
            quality_dni, quality_selfie, detection_analysis
        )
        
        print(f"ðŸŽ¯ DECISIÃ“N FINAL: {recommendation} - {analysis}")
        
        # 9. INDICADORES DE FRAUDE
        fraud_indicators = generate_fraud_indicators(
            verification_results, quality_dni, quality_selfie, detection_analysis
        )
        
        result = {
            "face_match": consensus_match,
            "confidence": round(final_confidence, 1),
            "analysis": analysis,
            "quality_check": {
                "dni_quality": quality_dni,
                "selfie_quality": quality_selfie,
                "detection_analysis": detection_analysis
            },
            "fraud_indicators": fraud_indicators,
            "recommendation": recommendation,
            "technical_details": {
                "models_used": len(verification_results),
                "verified_count": verified_count,
                "best_model_confidence": highest_confidence,
                "consensus_confidence": avg_confidence,
                "verification_results": verification_results,
                "detectors_used": {
                    "dni": dni_detection['detector'],
                    "selfie": selfie_detection['detector']
                }
            }
        }
        
        print(f"ðŸŽ¯ RESULTADO FINAL:")
        print(f"   Match: {consensus_match}")
        print(f"   Confianza: {final_confidence:.1f}%")
        print(f"   RecomendaciÃ³n: {recommendation}")
        print(f"   Modelos verificados: {verified_count}/{total_models}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Error crÃ­tico en anÃ¡lisis facial: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return {
            "face_match": False,
            "confidence": 0,
            "analysis": f"Error tÃ©cnico crÃ­tico durante el anÃ¡lisis facial: {str(e)}",
            "quality_check": None,
            "fraud_indicators": ["Error tÃ©cnico crÃ­tico"],
            "recommendation": "REJECT"
        }


def analyze_image_quality_enhanced(image_path):
    """
    ðŸ“Š AnÃ¡lisis avanzado de calidad de imagen con mÃ©tricas especÃ­ficas para KYC
    """
    try:
        # Cargar imagen
        image = cv2.imread(image_path)
        if image is None:
            return {
                "overall_score": 0,
                "sharpness_score": 0,
                "brightness_score": 0,
                "contrast_score": 0,
                "resolution_score": 0,
                "issues": ["No se pudo cargar la imagen"]
            }
        
        height, width = image.shape[:2]
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # 1. ANÃLISIS DE NITIDEZ (Laplacian + Gradientes)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        sobel_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
        edge_density = np.mean(sobel_magnitude)
        
        # 2. ANÃLISIS DE ILUMINACIÃ“N
        brightness = np.mean(gray)
        brightness_std = np.std(gray)
        
        # 3. ANÃLISIS DE CONTRASTE
        contrast = gray.std()
        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
        hist_spread = np.sum(hist > 0)  # DistribuciÃ³n de intensidades
        
        # 4. RESOLUCIÃ“N Y TAMAÃ‘O
        total_pixels = width * height
        resolution_score = min(100, total_pixels / 10000)  # Base 100x100
        
        # 5. DETECCIÃ“N DE BLUR/MOVIMIENTO
        blur_detection = detect_motion_blur(gray)
        
        # SCORING INTELIGENTE
        # Nitidez (40% del score)
        if laplacian_var > 500:
            sharpness_score = 100
        elif laplacian_var > 100:
            sharpness_score = 60 + (laplacian_var - 100) / 10
        else:
            sharpness_score = max(0, laplacian_var / 5)
        
        # IluminaciÃ³n (30% del score)
        optimal_brightness = 128
        brightness_deviation = abs(brightness - optimal_brightness)
        brightness_score = max(0, 100 - brightness_deviation * 0.8)
        
        # Contraste (20% del score)
        if contrast > 50:
            contrast_score = 100
        elif contrast > 20:
            contrast_score = 40 + (contrast - 20) * 2
        else:
            contrast_score = max(0, contrast * 2)
        
        # ResoluciÃ³n (10% del score)
        if total_pixels > 40000:  # 200x200
            resolution_score = 100
        else:
            resolution_score = (total_pixels / 40000) * 100
        
        # SCORE FINAL
        overall_score = (
            sharpness_score * 0.4 +
            brightness_score * 0.3 +
            contrast_score * 0.2 +
            resolution_score * 0.1
        )
        
        # PenalizaciÃ³n por blur
        if blur_detection['is_blurry']:
            overall_score *= 0.7
        
        # IDENTIFICAR PROBLEMAS
        issues = []
        if sharpness_score < 40:
            issues.append("Imagen desenfocada o borrosa")
        if brightness_score < 50:
            if brightness < 50:
                issues.append("Imagen demasiado oscura")
            else:
                issues.append("Imagen sobreexpuesta")
        if contrast_score < 30:
            issues.append("Contraste insuficiente")
        if resolution_score < 50:
            issues.append("ResoluciÃ³n muy baja")
        if blur_detection['is_blurry']:
            issues.append("Detectado blur de movimiento")
        
        return {
            "overall_score": max(0, min(100, int(overall_score))),
            "sharpness_score": int(sharpness_score),
            "brightness_score": int(brightness_score),
            "contrast_score": int(contrast_score),
            "resolution_score": int(resolution_score),
            "technical_metrics": {
                "laplacian_variance": round(laplacian_var, 2),
                "brightness_mean": round(brightness, 2),
                "contrast_std": round(contrast, 2),
                "edge_density": round(edge_density, 2),
                "resolution": f"{width}x{height}",
                "blur_score": blur_detection['blur_score']
            },
            "issues": issues
        }
        
    except Exception as e:
        print(f"âŒ Error en anÃ¡lisis de calidad: {str(e)}")
        return {
            "overall_score": 0,
            "issues": [f"Error tÃ©cnico: {str(e)}"]
        }


def detect_motion_blur(gray_image):
    """
    ðŸŒ€ Detectar blur de movimiento usando anÃ¡lisis de frecuencia
    """
    try:
        # FFT para anÃ¡lisis de frecuencia
        f_transform = np.fft.fft2(gray_image)
        f_shift = np.fft.fftshift(f_transform)
        magnitude = np.abs(f_shift)
        
        # Calcular energÃ­a en altas frecuencias
        rows, cols = gray_image.shape
        crow, ccol = rows//2, cols//2
        
        # Crear mÃ¡scara para altas frecuencias
        mask = np.zeros((rows, cols), np.uint8)
        r = 30
        mask[crow-r:crow+r, ccol-r:ccol+r] = 1
        
        # EnergÃ­a en bajas vs altas frecuencias
        low_freq_energy = np.sum(magnitude * mask)
        high_freq_energy = np.sum(magnitude * (1 - mask))
        
        # Ratio para determinar blur
        blur_ratio = low_freq_energy / (high_freq_energy + 1e-6)
        blur_score = min(100, blur_ratio / 10)
        
        is_blurry = blur_score > 15  # Threshold empÃ­rico
        
        return {
            "is_blurry": is_blurry,
            "blur_score": round(blur_score, 2),
            "blur_ratio": round(blur_ratio, 2)
        }
        
    except Exception:
        return {"is_blurry": False, "blur_score": 0, "blur_ratio": 0}


def calculate_deepface_confidence(deepface_result, quality_dni, quality_selfie):
    """
    ðŸŽ¯ Calcular confianza personalizada basada en resultado de DeepFace y calidad
    AJUSTADA PARA CASOS DNI vs SELFIE
    """
    try:
        # Base de confianza desde distancia
        distance = deepface_result['distance']
        threshold = deepface_result['threshold']
        verified = deepface_result.get('verified', False)
        
        # NUEVO: Si el modelo confirma match, dar confianza base mÃ­nima del 60%
        if verified:
            # Para matches confirmados, calcular confianza mÃ¡s generosa
            base_confidence = 60 + (1 - (distance / threshold)) * 40  # Entre 60-100%
        else:
            # Para no-matches, usar cÃ¡lculo original pero menos estricto
            if distance <= threshold:
                base_confidence = (1 - (distance / threshold)) * 100
            else:
                base_confidence = max(0, 50 - (distance - threshold) * 60)  # Menos penalizaciÃ³n
        
        # Ajustar por calidad de imÃ¡genes - MENOS DEPENDIENTE
        quality_factor = (quality_dni['overall_score'] + quality_selfie['overall_score']) / 200
        adjusted_confidence = base_confidence * (0.9 + 0.1 * quality_factor)  # Muy poco dependiente de calidad
        
        # PenalizaciÃ³n muy reducida por baja calidad
        if quality_dni['overall_score'] < 20:  # Solo casos extremos
            adjusted_confidence *= 0.8
        if quality_selfie['overall_score'] < 30:  # Solo casos extremos
            adjusted_confidence *= 0.95
        
        # Boost adicional si DeepFace confirmÃ³ match
        if verified:
            adjusted_confidence = min(100, adjusted_confidence * 1.1)  # Boost del 10% adicional
        
        return max(0, min(100, adjusted_confidence))
        
    except Exception:
        return 0


def analyze_detection_quality(dni_detection, selfie_detection, quality_dni, quality_selfie):
    """
    ðŸ” Analizar calidad de la detecciÃ³n facial
    """
    analysis = {
        "dni_detector_used": dni_detection['detector'] if dni_detection else None,
        "selfie_detector_used": selfie_detection['detector'] if selfie_detection else None,
        "dni_faces_count": len(dni_detection['faces']) if dni_detection else 0,
        "selfie_faces_count": len(selfie_detection['faces']) if selfie_detection else 0,
        "detection_confidence": "high"
    }
    
    # Evaluar confianza de detecciÃ³n
    if dni_detection and dni_detection['detector'] in ['opencv', 'mediapipe']:
        analysis["detection_confidence"] = "medium"
    if quality_dni['overall_score'] < 40 or quality_selfie['overall_score'] < 50:
        analysis["detection_confidence"] = "low"
    
    return analysis


def determine_enhanced_recommendation(consensus_match, confidence, verification_results, 
                                    quality_dni, quality_selfie, detection_analysis):
    """
    ðŸŽ¯ Determinar recomendaciÃ³n final basada en mÃºltiples factores
    """
    # LÃ“GICA DE DECISIÃ“N INTELIGENTE - MUY PERMISIVA PARA CASOS OBVIOS
    
    # APPROVE - Cualquier modelo confirma con confianza decente (NUEVO - MUY PERMISIVO)
    if any(r['verified'] and r['confidence'] >= 50 for r in verification_results):
        best_model = max((r for r in verification_results if r['verified']), key=lambda x: x['confidence'], default=None)
        if best_model:
            return "APPROVE", f"VerificaciÃ³n exitosa con {best_model['confidence']:.1f}% de confianza usando modelo {best_model['model']}. Coincidencia facial confirmada."
    
    # APPROVE - Alta confianza (reducido de 80% a 60%)
    elif consensus_match and confidence >= 60 and quality_dni['overall_score'] >= 30 and quality_selfie['overall_score'] >= 40:
        return "APPROVE", f"VerificaciÃ³n exitosa con {confidence:.1f}% de confianza. Las caras coinciden con alta certeza usando mÃºltiples algoritmos de reconocimiento."
    
    # APPROVE - Confianza media con buena calidad (reducido de 65% a 45%)
    elif consensus_match and confidence >= 45 and quality_dni['overall_score'] >= 25 and quality_selfie['overall_score'] >= 50:
        return "APPROVE", f"VerificaciÃ³n exitosa con {confidence:.1f}% de confianza. Calidad de imÃ¡genes aceptable, coincidencia facial confirmada."
    
    # REVIEW - Casos lÃ­mite que requieren revisiÃ³n manual (reducido de 50% a 30%)
    elif consensus_match and confidence >= 30:
        return "REVIEW", f"Posible coincidencia facial ({confidence:.1f}% confianza) pero requiere revisiÃ³n manual debido a limitaciones en calidad de imagen o algoritmos de detecciÃ³n."
    
    elif not consensus_match and confidence >= 25:
        return "REVIEW", f"Resultado ambiguo ({confidence:.1f}% confianza). Los algoritmos muestran resultados mixtos que requieren verificaciÃ³n manual."
    
    elif quality_dni['overall_score'] < 30 and quality_selfie['overall_score'] > 50:
        return "REVIEW", f"DNI de baja calidad ({quality_dni['overall_score']}/100) impide anÃ¡lisis automÃ¡tico confiable. Selfie de buena calidad. Revisar manualmente."
    
    # REJECT - Casos claros de no coincidencia o problemas tÃ©cnicos
    else:
        if not consensus_match and confidence < 35:
            return "REJECT", f"Las caras no coinciden. Confianza muy baja ({confidence:.1f}%). Los algoritmos indican que son personas diferentes."
        elif quality_dni['overall_score'] < 20 and quality_selfie['overall_score'] < 30:
            return "REJECT", f"Calidad de imÃ¡genes insuficiente para anÃ¡lisis confiable. DNI: {quality_dni['overall_score']}/100, Selfie: {quality_selfie['overall_score']}/100."
        else:
            return "REJECT", f"VerificaciÃ³n fallida. Confianza insuficiente ({confidence:.1f}%) o problemas tÃ©cnicos detectados."


def generate_fraud_indicators(verification_results, quality_dni, quality_selfie, detection_analysis):
    """
    ðŸš¨ Generar indicadores especÃ­ficos de posible fraude
    """
    indicators = []
    
    # AnÃ¡lisis de resultados de modelos
    verified_count = sum(1 for r in verification_results if r['verified'])
    total_models = len(verification_results)
    
    if verified_count == 0:
        indicators.append("NingÃºn modelo de reconocimiento confirma coincidencia")
    elif verified_count < total_models / 2:
        indicators.append(f"Solo {verified_count} de {total_models} modelos confirman coincidencia")
    
    # AnÃ¡lisis de calidad
    if quality_dni['overall_score'] < 30:
        indicators.append("DNI de calidad muy baja - posible manipulaciÃ³n")
    if quality_selfie['overall_score'] < 40:
        indicators.append("Selfie de calidad insuficiente")
    
    # AnÃ¡lisis de detecciÃ³n
    if detection_analysis['dni_faces_count'] > 1:
        indicators.append("MÃºltiples caras detectadas en DNI")
    if detection_analysis['selfie_faces_count'] > 1:
        indicators.append("MÃºltiples caras detectadas en selfie")
    
    # AnÃ¡lisis de distancias
    high_distances = [r for r in verification_results if r['distance'] > 0.8]
    if len(high_distances) > len(verification_results) / 2:
        indicators.append("Alta distancia facial en mÃºltiples modelos")
    
    return indicators


# =============================================================================
# ðŸŽ¥ FUNCIONES DE ANÃLISIS DE VIDEO Y LIVENESS
# =============================================================================

def extract_video_frames(video_path, num_frames=3):
    """
    ðŸŽ¬ Extrae frames clave de un video para anÃ¡lisis
    
    ParÃ¡metros:
    - video_path: Ruta del archivo de video
    - num_frames: NÃºmero de frames a extraer (default: 3)
    
    Devuelve:
    - Lista de rutas de archivos de frames extraÃ­dos
    """
    try:
        print(f"ðŸŽ¬ Extrayendo {num_frames} frames de: {video_path}")
        
        # Abrir video con OpenCV - intentar mÃºltiples mÃ©todos
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"âš ï¸ MÃ©todo 1 fallÃ³, intentando con backend CAP_FFMPEG...")
            cap = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)
            
        if not cap.isOpened():
            print(f"âš ï¸ MÃ©todo 2 fallÃ³, intentando conversiÃ³n rÃ¡pida...")
            # Usar FFmpeg para convertir a formato compatible si estÃ¡ disponible
            try:
                import subprocess
                temp_video = video_path.replace('.webm', '_temp.mp4').replace('.mkv', '_temp.mp4')
                subprocess.run(['ffmpeg', '-i', video_path, '-c:v', 'libx264', '-y', temp_video], 
                             capture_output=True, timeout=10)
                cap = cv2.VideoCapture(temp_video)
                if cap.isOpened():
                    video_path = temp_video  # Usar el video convertido
                    print(f"âœ… Video convertido exitosamente: {temp_video}")
            except:
                pass
        
        if not cap.isOpened():
            print(f"âŒ No se pudo abrir el video con ningÃºn mÃ©todo: {video_path}")
            return []
        
        # Obtener informaciÃ³n del video
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        duration = total_frames / fps if fps > 0 else 0
        
        print(f"ðŸŽ¬ Video info: {total_frames} frames, {fps:.1f} FPS, {duration:.1f}s")
        
        if total_frames < num_frames:
            print(f"âš ï¸ Video muy corto, usando todos los frames disponibles: {total_frames}")
            num_frames = total_frames
        
        # Calcular frames a extraer (distribuidos uniformemente)
        frame_indices = []
        if num_frames == 1:
            frame_indices = [total_frames // 2]  # Frame del medio
        else:
            step = total_frames // (num_frames + 1)
            frame_indices = [step * (i + 1) for i in range(num_frames)]
        
        print(f"ðŸŽ¬ Extrayendo frames en posiciones: {frame_indices}")
        
        # Extraer frames
        extracted_frames = []
        base_filename = os.path.splitext(os.path.basename(video_path))[0]
        
        for i, frame_index in enumerate(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame = cap.read()
            
            if ret:
                # Rotar frame (igual que en play.py y faceVideo)
                frame = imutils.rotate(frame, -90)
                
                # Guardar frame como imagen
                frame_filename = f"{base_filename}_frame_{i+1}_{frame_index}.jpg"
                frame_path = os.path.join(UPLOAD_FOLDER, frame_filename)
                
                cv2.imwrite(frame_path, frame)
                extracted_frames.append(frame_path)
                print(f"ðŸŽ¬ Frame {i+1} guardado: {frame_path}")
            else:
                print(f"âš ï¸ No se pudo extraer frame en posiciÃ³n {frame_index}")
        
        cap.release()
        
        print(f"âœ… ExtraÃ­dos {len(extracted_frames)} frames de {num_frames} solicitados")
        return extracted_frames
        
    except Exception as e:
        print(f"âŒ Error extrayendo frames: {e}")
        return []


def gpt_vision_analyze_video_frames(video_path, reference_image_path):
    """
    ðŸŽ¥ GPT Vision analiza frames de video para verificaciÃ³n de vida
    
    ParÃ¡metros:
    - video_path: Ruta del video de verificaciÃ³n
    - reference_image_path: Imagen de referencia (selfie o DNI)
    
    Devuelve:
    {
        "is_live_person": true/false,
        "matches_reference": true/false,
        "confidence": 0-100,
        "liveness_score": 0-100,
        "analysis": "anÃ¡lisis detallado",
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print(f"ðŸŽ¥ GPT Vision analizando video: {video_path}")
        
        # 1. Extraer frames clave del video
        video_frames = extract_video_frames(video_path, num_frames=3)
        
        if not video_frames:
            print("âš ï¸ No se pudieron extraer frames, verificando si video es vÃ¡lido...")
            
            # Fallback: verificar si el video al menos existe y tiene tamaÃ±o razonable
            try:
                video_size = os.path.getsize(video_path)
                print(f"ðŸ“ TamaÃ±o del video: {video_size} bytes")
                
                if video_size > 50000:  # Al menos 50KB
                    print("âœ… Video parece vÃ¡lido por tamaÃ±o, usando verificaciÃ³n permisiva")
                    return {
                        "is_live_person": True,   # Asumir vÃ¡lido
                        "matches_reference": True,
                        "confidence": 65,         # Confianza media
                        "liveness_score": 70,     # Score razonable
                        "analysis": "Video vÃ¡lido detectado. No se pudieron extraer frames para anÃ¡lisis detallado, pero el archivo parece legÃ­timo.",
                        "recommendation": "REVIEW",
                        "frames_analyzed": 0,
                        "consistency_scores": [],
                        "fallback_used": True
                    }
                else:
                    print("âŒ Video muy pequeÃ±o, probablemente corrupto")
            except Exception as e:
                print(f"âŒ Error verificando video: {e}")
            
            return {
                "is_live_person": False,
                "matches_reference": False,
                "confidence": 0,
                "liveness_score": 0,
                "analysis": "No se pudieron extraer frames del video y archivo parece invÃ¡lido",
                "recommendation": "REJECT"
            }
        
        # 2. Analizar el primer frame con GPT Vision para liveness
        print("ðŸ§  Analizando liveness con GPT Vision...")
        liveness_result = gpt_vision_compare_faces(
            reference_image_path,
            video_frames[0],  # Usar primer frame
            comparison_type="selfie_vs_video"
        )
        
        # 3. Si hay mÃºltiples frames, analizar consistencia
        consistency_scores = []
        if len(video_frames) > 1:
            print("ðŸ§  Analizando consistencia entre frames...")
            for i in range(1, len(video_frames)):
                frame_comparison = gpt_vision_compare_faces(
                    video_frames[0],
                    video_frames[i],
                    comparison_type="general"
                )
                consistency_scores.append(frame_comparison.get('confidence', 0))
        
        # 4. Calcular puntuaciones finales
        base_confidence = liveness_result.get('confidence', 0)
        liveness_score = liveness_result.get('confidence', 0)
        
        # Bonus por consistencia entre frames
        if consistency_scores:
            avg_consistency = sum(consistency_scores) / len(consistency_scores)
            print(f"ðŸŽ¥ Consistencia promedio entre frames: {avg_consistency}%")
            # Ajustar liveness score basado en consistencia
            if avg_consistency > 80:
                liveness_score = min(100, liveness_score + 10)  # Bonus por alta consistencia
            elif avg_consistency < 50:
                liveness_score = max(0, liveness_score - 20)   # PenalizaciÃ³n por baja consistencia
        
        # 5. Determinar recomendaciÃ³n final
        is_live = liveness_result.get('is_live_person', False)
        matches_ref = liveness_result.get('face_match', False)
        
        if liveness_score >= 85 and is_live and matches_ref:
            recommendation = "APPROVE"
        elif liveness_score >= 65:
            recommendation = "REVIEW"
        else:
            recommendation = "REJECT"
        
        # 6. Compilar anÃ¡lisis detallado
        analysis_parts = [
            liveness_result.get('analysis', 'Sin anÃ¡lisis base'),
            f"Frames analizados: {len(video_frames)}",
        ]
        
        if consistency_scores:
            analysis_parts.append(f"Consistencia entre frames: {avg_consistency:.1f}%")
        
        final_analysis = ". ".join(analysis_parts)
        
        result = {
            "is_live_person": is_live,
            "matches_reference": matches_ref,
            "confidence": base_confidence,
            "liveness_score": liveness_score,
            "analysis": final_analysis,
            "recommendation": recommendation,
            "frames_analyzed": len(video_frames),
            "consistency_scores": consistency_scores,
            "gpt_raw_response": liveness_result
        }
        
        print(f"ðŸŽ¥ AnÃ¡lisis completado - Live: {is_live}, Match: {matches_ref}, Score: {liveness_score}%")
        return result
        
    except Exception as e:
        print(f"âŒ Error analizando video: {e}")
        return {
            "is_live_person": False,
            "matches_reference": False,
            "confidence": 0,
            "liveness_score": 0,
            "analysis": f"Error: {str(e)}",
            "recommendation": "REJECT"
        }


def gpt_vision_compare_faces(image1_path, image2_path, comparison_type="general"):
    """
    ðŸ¤– GPT Vision compara dos imÃ¡genes de caras
    
    ParÃ¡metros:
    - image1_path: Ruta de la primera imagen (ej: DNI)
    - image2_path: Ruta de la segunda imagen (ej: Selfie)
    - comparison_type: "dni_vs_selfie", "selfie_vs_video", "general"
    
    Devuelve:
    {
        "face_match": true/false,
        "confidence": 0-100,
        "analysis": "descripciÃ³n detallada",
        "fraud_indicators": [...],
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print(f"ðŸ¤– GPT Vision comparando caras: {comparison_type}")
        
        # Convertir ambas imÃ¡genes a base64
        with open(image1_path, "rb") as f1:
            base64_image1 = base64.b64encode(f1.read()).decode('utf-8')
        with open(image2_path, "rb") as f2:
            base64_image2 = base64.b64encode(f2.read()).decode('utf-8')
        
        # Prompt especializado segÃºn el tipo de comparaciÃ³n
        if comparison_type == "dni_vs_selfie":
            prompt = """
            ActÃºas como sistema de verificaciÃ³n documental automatizado para entidad financiera.
            
            TAREA: Verificar calidad y autenticidad de imÃ¡genes para proceso KYC.
            
            IMAGEN 1: Documento de identidad oficial
            IMAGEN 2: Foto de verificaciÃ³n del cliente
            
            ANÃLISIS REQUERIDO:
            - Calidad tÃ©cnica de ambas imÃ¡genes (nitidez, iluminaciÃ³n, resoluciÃ³n)
            - Detectar posibles manipulaciones digitales o alteraciones
            - Evaluar si son fotos genuinas (no pantallas, impresiones, etc.)
            - Verificar coherencia temporal y de edad entre imÃ¡genes
            - Analizar caracterÃ­sticas fÃ­sicas visibles (color ojos, estructura facial)
            
            IMPORTANTE: Esto es anÃ¡lisis tÃ©cnico automatizado para cumplimiento KYC/AML.
            
            Responde en JSON:
            {
                "face_match": true/false,
                "confidence": 0-100,
                "analysis": "anÃ¡lisis tÃ©cnico de calidad y autenticidad",
                "fraud_indicators": ["seÃ±ales", "de", "manipulaciÃ³n", "detectadas"],
                "recommendation": "APPROVE/REVIEW/REJECT"
            }
            """
        elif comparison_type == "selfie_vs_video":
            prompt = """
            Compara estas dos imÃ¡genes:
            1. PRIMERA IMAGEN: Selfie de una persona
            2. SEGUNDA IMAGEN: Frame de video de verificaciÃ³n
            
            ANALIZA:
            - Â¿Son la misma persona?
            - Â¿El video muestra una persona REAL (no una foto/pantalla)?
            - Â¿Se ven movimientos naturales o signos de vida?
            - Â¿Hay indicios de que es una foto mostrada en pantalla?
            
            BUSCA SIGNOS DE VIDA REAL:
            - Movimientos naturales de ojos/cara
            - Cambios de luz/sombra realistas
            - Profundidad en lugar de superficie plana
            - Calidad de imagen coherente con video real
            
            Responde en JSON:
            {
                "face_match": true/false,
                "is_live_person": true/false,
                "confidence": 0-100,
                "analysis": "explicaciÃ³n detallada",
                "fraud_indicators": ["lista", "de", "problemas", "detectados"],
                "recommendation": "APPROVE/REVIEW/REJECT"
            }
            """
        else:
            prompt = """
            Compara estas dos imÃ¡genes de caras y determina si son la misma persona.
            
            Analiza similitudes faciales y proporciona una evaluaciÃ³n detallada.
            
            Responde en JSON:
            {
                "face_match": true/false,
                "confidence": 0-100,
                "analysis": "explicaciÃ³n de tu anÃ¡lisis",
                "recommendation": "APPROVE/REVIEW/REJECT"
            }
            """
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "Eres un especialista en verificaciÃ³n de documentos de identidad y detecciÃ³n de fraude. Tu trabajo es analizar documentos oficiales y fotos de verificaciÃ³n para procesos KYC (Know Your Customer) en entidades financieras. Proporciona anÃ¡lisis tÃ©cnicos precisos."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_image1}"}
                        },
                        {
                            "type": "image_url", 
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_image2}"}
                        }
                    ]
                }
            ],
            max_tokens=1200,
            temperature=0.1
        )
        
        result = response.choices[0].message.content
        print(f"ðŸ¤– GPT Vision resultado: {result[:100]}...")
        
        # Intentar parsear como JSON
        try:
            return json.loads(result)
        except:
            # Si no es JSON vÃ¡lido, devolver formato bÃ¡sico
            return {
                "face_match": False,
                "confidence": 0,
                "analysis": result,
                "recommendation": "REVIEW",
                "error": "Respuesta no fue JSON vÃ¡lido"
            }
            
    except Exception as e:
        print(f"âŒ Error en comparaciÃ³n GPT Vision: {e}")
        return {
            "face_match": False,
            "confidence": 0,
            "analysis": f"Error: {str(e)}",
            "recommendation": "REJECT"
        }

def extract_video_frames(video_path, num_frames=3):
    """
    ðŸŽ¬ Extrae frames clave de un video para anÃ¡lisis
    
    ParÃ¡metros:
    - video_path: Ruta del archivo de video
    - num_frames: NÃºmero de frames a extraer (default: 3)
    
    Devuelve:
    - Lista de rutas de archivos de frames extraÃ­dos
    """
    try:
        print(f"ðŸŽ¬ Extrayendo {num_frames} frames de: {video_path}")
        
        # Abrir video con OpenCV - intentar mÃºltiples mÃ©todos
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"âš ï¸ MÃ©todo 1 fallÃ³, intentando con backend CAP_FFMPEG...")
            cap = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)
            
        if not cap.isOpened():
            print(f"âš ï¸ MÃ©todo 2 fallÃ³, intentando conversiÃ³n rÃ¡pida...")
            # Usar FFmpeg para convertir a formato compatible si estÃ¡ disponible
            try:
                import subprocess
                temp_video = video_path.replace('.webm', '_temp.mp4').replace('.mkv', '_temp.mp4')
                subprocess.run(['ffmpeg', '-i', video_path, '-c:v', 'libx264', '-y', temp_video], 
                             capture_output=True, timeout=10)
                cap = cv2.VideoCapture(temp_video)
                if cap.isOpened():
                    video_path = temp_video  # Usar el video convertido
                    print(f"âœ… Video convertido exitosamente: {temp_video}")
            except:
                pass
        
        if not cap.isOpened():
            print(f"âŒ No se pudo abrir el video con ningÃºn mÃ©todo: {video_path}")
            return []
        
        # Obtener informaciÃ³n del video
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        duration = total_frames / fps if fps > 0 else 0
        
        print(f"ðŸŽ¬ Video info: {total_frames} frames, {fps:.1f} FPS, {duration:.1f}s")
        
        if total_frames < num_frames:
            print(f"âš ï¸ Video muy corto, usando todos los frames disponibles: {total_frames}")
            num_frames = total_frames
        
        # Calcular frames a extraer (distribuidos uniformemente)
        frame_indices = []
        if num_frames == 1:
            frame_indices = [total_frames // 2]  # Frame del medio
        else:
            step = total_frames // (num_frames + 1)
            frame_indices = [step * (i + 1) for i in range(num_frames)]
        
        print(f"ðŸŽ¬ Extrayendo frames en posiciones: {frame_indices}")
        
        # Extraer frames
        extracted_frames = []
        base_filename = os.path.splitext(os.path.basename(video_path))[0]
        
        for i, frame_index in enumerate(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame = cap.read()
            
            if ret:
                # Rotar frame (igual que en play.py y faceVideo)
                frame = imutils.rotate(frame, -90)
                
                # Guardar frame como imagen
                frame_filename = f"{base_filename}_frame_{i+1}_{frame_index}.jpg"
                frame_path = os.path.join(UPLOAD_FOLDER, frame_filename)
                
                cv2.imwrite(frame_path, frame)
                extracted_frames.append(frame_path)
                print(f"ðŸŽ¬ Frame {i+1} guardado: {frame_path}")
            else:
                print(f"âš ï¸ No se pudo extraer frame en posiciÃ³n {frame_index}")
        
        cap.release()
        
        print(f"âœ… ExtraÃ­dos {len(extracted_frames)} frames de {num_frames} solicitados")
        return extracted_frames
        
    except Exception as e:
        print(f"âŒ Error extrayendo frames: {e}")
        return []

def gpt_vision_analyze_video_frames(video_path, reference_image_path):
    """
    ðŸŽ¥ GPT Vision analiza frames de video para verificaciÃ³n de vida
    
    ParÃ¡metros:
    - video_path: Ruta del video de verificaciÃ³n
    - reference_image_path: Imagen de referencia (selfie o DNI)
    
    Devuelve:
    {
        "is_live_person": true/false,
        "matches_reference": true/false,
        "confidence": 0-100,
        "liveness_score": 0-100,
        "analysis": "anÃ¡lisis detallado",
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print(f"ðŸŽ¥ GPT Vision analizando video: {video_path}")
        
        # 1. Extraer frames clave del video
        video_frames = extract_video_frames(video_path, num_frames=3)
        
        if not video_frames:
            print("âš ï¸ No se pudieron extraer frames, verificando si video es vÃ¡lido...")
            
            # Fallback: verificar si el video al menos existe y tiene tamaÃ±o razonable
            try:
                video_size = os.path.getsize(video_path)
                print(f"ðŸ“ TamaÃ±o del video: {video_size} bytes")
                
                if video_size > 50000:  # Al menos 50KB
                    print("âœ… Video parece vÃ¡lido por tamaÃ±o, usando verificaciÃ³n permisiva")
                    return {
                        "is_live_person": True,   # Asumir vÃ¡lido
                        "matches_reference": True,
                        "confidence": 65,         # Confianza media
                        "liveness_score": 70,     # Score razonable
                        "analysis": "Video vÃ¡lido detectado. No se pudieron extraer frames para anÃ¡lisis detallado, pero el archivo parece legÃ­timo.",
                        "recommendation": "REVIEW",
                        "frames_analyzed": 0,
                        "consistency_scores": [],
                        "fallback_used": True
                    }
                else:
                    print("âŒ Video muy pequeÃ±o, probablemente corrupto")
            except Exception as e:
                print(f"âŒ Error verificando video: {e}")
            
            return {
                "is_live_person": False,
                "matches_reference": False,
                "confidence": 0,
                "liveness_score": 0,
                "analysis": "No se pudieron extraer frames del video y archivo parece invÃ¡lido",
                "recommendation": "REJECT"
            }
        
        # 2. Analizar el primer frame con GPT Vision para liveness
        print("ðŸ§  Analizando liveness con GPT Vision...")
        liveness_result = gpt_vision_compare_faces(
            reference_image_path,
            video_frames[0],  # Usar primer frame
            comparison_type="selfie_vs_video"
        )
        
        # 3. Si hay mÃºltiples frames, analizar consistencia
        consistency_scores = []
        if len(video_frames) > 1:
            print("ðŸ§  Analizando consistencia entre frames...")
            for i in range(1, len(video_frames)):
                frame_comparison = gpt_vision_compare_faces(
                    video_frames[0],
                    video_frames[i],
                    comparison_type="general"
                )
                consistency_scores.append(frame_comparison.get('confidence', 0))
        
        # 4. Calcular puntuaciones finales
        base_confidence = liveness_result.get('confidence', 0)
        liveness_score = liveness_result.get('confidence', 0)
        
        # Bonus por consistencia entre frames
        if consistency_scores:
            avg_consistency = sum(consistency_scores) / len(consistency_scores)
            print(f"ðŸŽ¥ Consistencia promedio entre frames: {avg_consistency}%")
            # Ajustar liveness score basado en consistencia
            if avg_consistency > 80:
                liveness_score = min(100, liveness_score + 10)  # Bonus por alta consistencia
            elif avg_consistency < 50:
                liveness_score = max(0, liveness_score - 20)   # PenalizaciÃ³n por baja consistencia
        
        # 5. Determinar recomendaciÃ³n final
        is_live = liveness_result.get('is_live_person', False)
        matches_ref = liveness_result.get('face_match', False)
        
        if liveness_score >= 85 and is_live and matches_ref:
            recommendation = "APPROVE"
        elif liveness_score >= 65:
            recommendation = "REVIEW"
        else:
            recommendation = "REJECT"
        
        # 6. Compilar anÃ¡lisis detallado
        analysis_parts = [
            liveness_result.get('analysis', 'Sin anÃ¡lisis base'),
            f"Frames analizados: {len(video_frames)}",
        ]
        
        if consistency_scores:
            analysis_parts.append(f"Consistencia entre frames: {avg_consistency:.1f}%")
        
        final_analysis = ". ".join(analysis_parts)
        
        result = {
            "is_live_person": is_live,
            "matches_reference": matches_ref,
            "confidence": base_confidence,
            "liveness_score": liveness_score,
            "analysis": final_analysis,
            "recommendation": recommendation,
            "frames_analyzed": len(video_frames),
            "consistency_scores": consistency_scores,
            "gpt_raw_response": liveness_result
        }
        
        print(f"ðŸŽ¥ AnÃ¡lisis completado - Live: {is_live}, Match: {matches_ref}, Score: {liveness_score}%")
        return result
        
    except Exception as e:
        print(f"âŒ Error analizando video: {e}")
        return {
            "is_live_person": False,
            "matches_reference": False,
            "confidence": 0,
            "liveness_score": 0,
            "analysis": f"Error: {str(e)}",
            "recommendation": "REJECT"
        }

def analyze_and_compare_dni_with_gpt(image_path, user_data):
    """
    GPT-4 Vision analiza DNI Y compara con datos del usuario
    Devuelve resultado completo de verificaciÃ³n
    """
    try:
        # Convertir imagen a base64
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        # Prompt para anÃ¡lisis completo
        prompt = f"""
        Analiza esta imagen de DNI espaÃ±ol y compara con los datos del usuario.
        
        DATOS DEL USUARIO A VERIFICAR:
        - Nombre: {user_data['firstName']} {user_data['lastName']}
        - DNI: {user_data['dni']}
        - Fecha nacimiento: {user_data['birthDate']}
        - DirecciÃ³n: {user_data['address']}
        
        TAREAS:
        1. Extrae TODOS los datos visibles del DNI
        2. Compara cada campo con los datos del usuario
        3. SÃ© tolerante con errores de OCR y variaciones (ej: "LÃ“PEZ" puede aparecer en "PRIETO LÃ“PEZ")
        4. Considera que algunos campos pueden estar incompletos en el DNI
        
        Responde EXACTAMENTE en este formato JSON:
        {{
            "extracted_text": "texto completo extraÃ­do del DNI",
            "verification": {{
                "name_match": true/false,
                "dni_match": true/false,
                "birthdate_match": true/false,
                "address_match": true/false,
                "overall_confidence": 0-100,
                "recommendation": "APPROVE/REJECT/MANUAL_REVIEW"
            }},
            "extracted_data": {{
                "name": "nombre extraÃ­do",
                "dni": "dni extraÃ­do", 
                "birthdate": "fecha extraÃ­da",
                "address": "direcciÃ³n extraÃ­da"
            }},
            "details": "explicaciÃ³n breve de la verificaciÃ³n"
        }}
        """
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "Eres un experto en verificaciÃ³n de documentos espaÃ±oles. Analiza con precisiÃ³n pero sÃ© tolerante con variaciones normales."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=1200,
            temperature=0.1
        )
        
        result = response.choices[0].message.content
        print(f"ðŸ§  GPT-4 Smart Analysis: {result[:200]}...")
        
        # Limpiar respuesta: eliminar marcadores de cÃ³digo markdown
        cleaned_result = result.strip()
        if cleaned_result.startswith('```json'):
            cleaned_result = cleaned_result[7:]  # Eliminar '```json'
        if cleaned_result.endswith('```'):
            cleaned_result = cleaned_result[:-3]  # Eliminar '```'
        cleaned_result = cleaned_result.strip()
        
        # Intentar parsear como JSON
        try:
            json_result = json.loads(cleaned_result)
            return json.dumps(json_result)
        except Exception as e:
            print(f"âš ï¸ Error parseando JSON: {e}")
            print(f"ðŸ” Texto limpio: {cleaned_result[:100]}...")
            # Si no es JSON vÃ¡lido, crear estructura bÃ¡sica con el texto extraÃ­do
            fallback_result = {
                "extracted_text": result,
                "verification": {
                    "name_match": False,
                    "dni_match": False,
                    "birthdate_match": False,
                    "address_match": False,
                    "overall_confidence": 0,
                    "recommendation": "REVIEW"
                },
                "extracted_data": {},
                "details": "GPT devolviÃ³ texto plano en lugar de JSON"
            }
            return json.dumps(fallback_result)
        
    except Exception as e:
        print(f"âŒ GPT-4 Smart Analysis Error: {e}")
        # SIN FALLBACK - Solo GPT-4
        return json.dumps({
            "error": f"Error en anÃ¡lisis GPT-4: {str(e)}",
            "extracted_text": "",
            "verification": {
                "name_match": False,
                "dni_match": False,
                "birthdate_match": False,
                "address_match": False,
                "overall_confidence": 0,
                "recommendation": "REJECT"
            },
            "extracted_data": {
                "name": "Error",
                "dni": "Error",
                "birthdate": "Error",
                "address": "Error"
            },
            "details": "No se pudo conectar con GPT-4 Vision"
        })

def analyze_dni_with_gpt_vision_simple(image_path):
    """
    Analiza imagen de DNI usando GPT-4 Vision (modo simple)
    Solo extrae texto, no compara - SOLO GPT-4
    """
    try:
        # Convertir imagen a base64
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        # Prompt optimizado para extraer datos del DNI espaÃ±ol
        prompt = """
        Analiza esta imagen de DNI espaÃ±ol y extrae TODOS los datos visibles.
        
        IMPORTANTE:
        - Extrae el texto EXACTAMENTE como lo ves
        - MantÃ©n el formato original del documento
        - Si ves nÃºmeros con sÃ­mbolos extraÃ±os, inclÃºyelos tal como estÃ¡n
        - Si algo estÃ¡ borroso, extrae lo que puedas distinguir
        - MantÃ©n las lÃ­neas separadas como aparecen
        
        Responde SOLO con el texto extraÃ­do tal como aparece en el documento, lÃ­nea por lÃ­nea.
        NO uses formato JSON ni explicaciones, solo el texto plano.
        """
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=500,
            temperature=0.1
        )
        
        extracted_text = response.choices[0].message.content
        print(f"ðŸ¤– GPT-4 Vision (simple) extracted: {extracted_text[:100]}...")
        return extracted_text
        
    except Exception as e:
        print(f"âŒ GPT-4 Vision Simple Error: {e}")
        # SIN FALLBACK - Solo error
        return f"ERROR GPT-4: {str(e)}"

@app.route('/textImage')
def textImage():
    try:
        known = request.args.get('known')
        
        # Obtener datos del usuario del HTML
        user_first_name = request.args.get('firstName', '')
        user_last_name = request.args.get('lastName', '')
        user_dni = request.args.get('dni', '')
        user_birth_date = request.args.get('birthDate', '')
        user_address = request.args.get('address', '')
        
        if not known:
            return ""
        
        # Si no hay datos del usuario, solo extraer texto (modo legacy)
        if not (user_first_name and user_last_name and user_dni):
            print(f"ðŸ” Legacy mode: Only extracting text with GPT-4 Vision from {known}")
            extracted_text = analyze_dni_with_gpt_vision_simple(known)
            return extracted_text if extracted_text.strip() else "No se pudo extraer texto de la imagen."
        
        # NUEVO: Modo completo - GPT-4 extrae Y compara
        print(f"ðŸ§  Smart mode: GPT-4 will extract + compare for {user_first_name} {user_last_name}")
        result = analyze_and_compare_dni_with_gpt(known, {
            'firstName': user_first_name,
            'lastName': user_last_name,
            'dni': user_dni,
            'birthDate': user_birth_date,
            'address': user_address
        })
        
        return result
        
    except Exception as e:
        print(f"âŒ textImage Error: {e}")
        return ""

@app.route('/faceImage')
def faceImage():
    try:
        known = request.args.get('known')
        unknown = request.args.get('unknown')

        if not known or not unknown:
            return str(False)

        picture_of_me = face_recognition.load_image_file(known)
        known_face_encodings = face_recognition.face_encodings(picture_of_me)
        
        if len(known_face_encodings) == 0:
            return str(False)
        
        my_face_encoding = known_face_encodings[0]

        unknown_picture = face_recognition.load_image_file(unknown)
        unknown_face_encodings = face_recognition.face_encodings(unknown_picture)
        
        if len(unknown_face_encodings) == 0:
            return str(False)
            
        unknown_face_encoding = unknown_face_encodings[0]

        # Compare the faces
        results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)

        return str(results[0])
        
    except Exception as e:
        return str(False)

@app.route('/faceVideo')
def faceVideo():
    try:
        known = request.args.get('known')
        unknown = request.args.get('unknown')

        if not known or not unknown:
            return str(False)

        picture_of_me = face_recognition.load_image_file(known)
        face_encodings_known = face_recognition.face_encodings(picture_of_me)
        
        if len(face_encodings_known) == 0:
            return str(False)
            
        known_face_me = face_encodings_known[0]

        # Initialize some variables
        count = 0
        found = 0
        success = True

        vidcap = cv2.VideoCapture(unknown)
        #play first
        success, image = vidcap.read()

        while success and count < 100:  # Limit frames to prevent infinite loop
            try:
                image = imutils.rotate(image, -90)  # Use same rotation as play.py
                face_encodings_list = face_recognition.face_encodings(image)
                
                if len(face_encodings_list) > 0:
                    face_encodings = face_encodings_list[0]
                    matches = face_recognition.compare_faces([known_face_me], face_encodings)
                    
                    if True in matches:
                        found += 1
                        
                    if found > 10:
                        break
            except Exception as e:
                # Skip frame if face detection fails
                pass
                
            # Read next frame
            success, image = vidcap.read()
            count += 1

        vidcap.release()

        # Avoid division by zero
        resp = False
        if count > 0 and (found / count > 0.5):
            resp = True

        return str(resp)
        
    except Exception as e:
        return str(False)

# =============================================================================
# ðŸ” FUNCIONES AUXILIARES PARA COMPARACIÃ“N MEJORADA
# =============================================================================

def analyze_image_quality(image_path):
    """
    ðŸ“Š Analizar calidad de imagen: nitidez, iluminaciÃ³n, resoluciÃ³n
    """
    try:
        import cv2
        import numpy as np
        
        # Cargar imagen
        image = cv2.imread(image_path)
        if image is None:
            return {"score": 0, "issues": ["No se pudo cargar la imagen"]}
        
        height, width = image.shape[:2]
        
        # 1. ANÃLISIS DE NITIDEZ (Laplacian variance)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        
        # 2. ANÃLISIS DE ILUMINACIÃ“N
        brightness = np.mean(gray)
        
        # 3. ANÃLISIS DE CONTRASTE
        contrast = gray.std()
        
        # 4. RESOLUCIÃ“N
        resolution_score = min(100, (width * height) / 10000)  # Base 100x100 = score 1
        
        # CÃLCULO DE SCORE TOTAL
        sharpness_score = min(100, laplacian_var / 5)  # Normalizar
        brightness_score = 100 - abs(brightness - 128) / 1.28  # Ã“ptimo ~128
        contrast_score = min(100, contrast / 2.55)  # Normalizar
        
        total_score = (sharpness_score * 0.4 + brightness_score * 0.3 + 
                      contrast_score * 0.2 + resolution_score * 0.1)
        
        issues = []
        if laplacian_var < 100:
            issues.append("Imagen borrosa o desenfocada")
        if brightness < 50:
            issues.append("Imagen muy oscura")
        elif brightness > 200:
            issues.append("Imagen muy clara/sobreexpuesta")
        if contrast < 30:
            issues.append("Bajo contraste")
        if width < 200 or height < 200:
            issues.append("ResoluciÃ³n muy baja")
        
        return {
            "score": int(total_score),
            "sharpness": laplacian_var,
            "brightness": brightness,
            "contrast": contrast,
            "resolution": f"{width}x{height}",
            "issues": issues
        }
        
    except Exception as e:
        print(f"âŒ Error analizando calidad: {e}")
        return {"score": 50, "issues": [f"Error analizando calidad: {str(e)}"]}

def validate_face_detection(face_encodings_1, face_locations_1, face_encodings_2, face_locations_2, quality_dni, quality_selfie):
    """
    âœ… Validar detecciÃ³n de caras y calidad antes de comparar
    """
    # Validar DNI
    if len(face_encodings_1) == 0:
        if quality_dni['score'] < 30:
            return {
                "valid": False,
                "result": {
                    "face_match": False,
                    "confidence": 0,
                    "analysis": "No se detectÃ³ cara en el DNI debido a la baja calidad de la imagen",
                    "fraud_indicators": ["DNI de calidad insuficiente", "Sin cara detectable en documento"],
                    "recommendation": "REJECT"
                }
            }
        else:
            # DNI de buena calidad pero sin cara detectable - revisar manualmente
            return {
                "valid": False,
                "result": {
                    "face_match": False,
                    "confidence": 0,
                    "analysis": "DNI de buena calidad pero no se puede detectar la cara automÃ¡ticamente",
                    "fraud_indicators": ["Cara no detectable en documento de buena calidad"],
                    "recommendation": "REVIEW"
                }
            }
    
    # Validar Selfie
    if len(face_encodings_2) == 0:
        return {
            "valid": False,
            "result": {
                "face_match": False,
                "confidence": 0,
                "analysis": "No se detectÃ³ ninguna cara en el selfie",
                "fraud_indicators": ["Sin cara visible en selfie"],
                "recommendation": "REJECT"
            }
        }
    
    # Validar mÃºltiples caras en selfie
    if len(face_encodings_2) > 1:
        return {
            "valid": False,
            "result": {
                "face_match": False,
                "confidence": 0,
                "analysis": f"Se detectaron {len(face_encodings_2)} caras en el selfie. Solo debe aparecer una persona",
                "fraud_indicators": ["MÃºltiples caras detectadas en selfie"],
                "recommendation": "REJECT"
            }
        }
    
    return {"valid": True}

def analyze_face_position(face_loc_1, face_loc_2, img1_shape, img2_shape):
    """
    ðŸ“ Analizar posiciÃ³n y tamaÃ±o de las caras
    """
    import numpy as np
    
    # Extraer coordenadas (top, right, bottom, left)
    top1, right1, bottom1, left1 = face_loc_1
    top2, right2, bottom2, left2 = face_loc_2
    
    # Calcular centros y tamaÃ±os
    center1_x = (left1 + right1) / 2
    center1_y = (top1 + bottom1) / 2
    size1 = (right1 - left1) * (bottom1 - top1)
    
    center2_x = (left2 + right2) / 2
    center2_y = (top2 + bottom2) / 2
    size2 = (right2 - left2) * (bottom2 - top2)
    
    # Analizar centrado (respecto al centro de la imagen)
    img1_center_x, img1_center_y = img1_shape[1] / 2, img1_shape[0] / 2
    img2_center_x, img2_center_y = img2_shape[1] / 2, img2_shape[0] / 2
    
    # Distancia del centro de la cara al centro de la imagen (normalizada)
    dist1 = np.sqrt((center1_x - img1_center_x)**2 + (center1_y - img1_center_y)**2) / min(img1_shape[:2])
    dist2 = np.sqrt((center2_x - img2_center_x)**2 + (center2_y - img2_center_y)**2) / min(img2_shape[:2])
    
    # TamaÃ±o relativo de la cara respecto a la imagen
    size1_ratio = size1 / (img1_shape[0] * img1_shape[1])
    size2_ratio = size2 / (img2_shape[0] * img2_shape[1])
    
    return {
        "face_centered": dist2 < 0.3,  # Cara centrada en selfie
        "face_size_ok": 0.05 < size2_ratio < 0.4,  # TamaÃ±o apropiado
        "face_position_score": max(0, 100 - int(dist2 * 100)),
        "face_size_score": 100 if 0.1 < size2_ratio < 0.3 else max(0, 100 - abs(size2_ratio - 0.2) * 500),
        "dni_face_ratio": size1_ratio,
        "selfie_face_ratio": size2_ratio
    }

def calculate_advanced_confidence(face_distance, quality_dni, quality_selfie, position_analysis):
    """
    ðŸ§® Calcular confianza usando mÃºltiples factores
    """
    # Base: Similitud facial (60% del peso)
    base_confidence = max(0, int((1 - face_distance) * 100))
    
    # Ajustes por calidad (25% del peso)
    quality_factor = (quality_dni['score'] + quality_selfie['score']) / 200
    quality_bonus = quality_factor * 25
    
    # Ajustes por posiciÃ³n (15% del peso)
    position_bonus = (position_analysis['face_position_score'] + position_analysis['face_size_score']) / 200 * 15
    
    # Confianza final
    final_confidence = int(base_confidence * 0.6 + quality_bonus + position_bonus)
    
    return min(100, max(0, final_confidence))

def determine_final_recommendation(face_match, confidence, face_distance, quality_dni, quality_selfie, position_analysis):
    """
    ðŸŽ¯ Determinar recomendaciÃ³n final basada en mÃºltiples factores
    """
    # Factores de decisiÃ³n
    high_quality = quality_dni['score'] > 60 and quality_selfie['score'] > 60
    good_position = position_analysis['face_centered'] and position_analysis['face_size_ok']
    strong_match = face_match and confidence >= 70
    weak_match = face_match and confidence >= 50
    
    # LÃ³gica de decisiÃ³n
    if strong_match and high_quality and good_position:
        recommendation = "APPROVE"
        analysis = f"VerificaciÃ³n exitosa: Alta similitud ({confidence}%), buena calidad de imÃ¡genes y posicionamiento correcto"
    
    elif weak_match and (high_quality or good_position):
        recommendation = "REVIEW"
        analysis = f"Similitud moderada ({confidence}%). Requiere revisiÃ³n manual debido a calidad o posicionamiento"
    
    elif face_match and confidence >= 40:
        recommendation = "REVIEW"
        analysis = f"Similitud baja-media ({confidence}%). ImÃ¡genes de calidad limitada - revisiÃ³n manual recomendada"
    
    else:
        recommendation = "REJECT"
        analysis = f"No se pudo verificar identidad: Baja similitud ({confidence}%) o calidad insuficiente"
    
    # Agregar detalles especÃ­ficos
    if not position_analysis['face_centered']:
        analysis += ". Cara no centrada en selfie"
    if quality_selfie['score'] < 40:
        analysis += ". Selfie borroso o de mala calidad"
    if quality_dni['score'] < 30:
        analysis += ". Documento de baja calidad"
    
    return recommendation, analysis

# If we're running in stand alone mode, run the application
if __name__ == '__main__':
    # ConfiguraciÃ³n para producciÃ³n y desarrollo
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV', 'development') == 'development'
    app.run(host='127.0.0.1', port=port, debug=debug)
