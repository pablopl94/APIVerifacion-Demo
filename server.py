from flask import (
    Flask,
    request,
    render_template,
    jsonify,
    send_from_directory
)
import face_recognition
import array as arr
import cv2
import os
from pathlib import Path
import imutils
try:
    from PIL import Image
except ImportError:
    import Image
import uuid
from werkzeug.utils import secure_filename
import base64
import json
import re
from openai import OpenAI
from dotenv import load_dotenv
import numpy as np
from deepface import DeepFace
from mtcnn.mtcnn import MTCNN
import warnings
warnings.filterwarnings('ignore')

# Cargar variables de entorno FORZANDO override
load_dotenv(override=True)

# FORZAR lectura directa del archivo .env
with open('.env', 'r') as f:
    for line in f:
        if line.startswith('OPENAI_API_KEY='):
            api_key_loaded = line.split('=', 1)[1].strip()
            break
    else:
        api_key_loaded = None

print(f"üîë API Key del .env: {api_key_loaded[:15] if api_key_loaded else 'NO ENCONTRADA'}...{api_key_loaded[-4:] if api_key_loaded else ''}")

# Inicializar cliente OpenAI
client = OpenAI(api_key=api_key_loaded)

# Create the application instance
app = Flask(__name__, 
           template_folder="templates",
           static_folder="static",
           static_url_path="/static")

# Configuration
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size
UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'mp4', 'avi', 'mov', 'mkv', 'webm'}

# Create upload directory if it doesn't exist
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# Create a URL route in our application for "/"
@app.route('/')
def index():
    return 'API FUNCIONANDO'

# New route to serve the KYC verification flow (original)
@app.route('/test')
def kyc_verification():
    return render_template('kyc_verification.html')

# New professional KYC verification route
@app.route('/kyc')
def kyc_verification_professional():
    return render_template('kyc_verification_new.html')

# Upload endpoint - handles both images and videos
@app.route('/upload', methods=['POST'])
def upload_file():
    try:
        # Check for files in any of these fields
        file = None
        if 'image' in request.files and request.files['image'].filename != '':
            file = request.files['image']
        elif 'video' in request.files and request.files['video'].filename != '':
            file = request.files['video']
        
        if not file:
            return jsonify({'error': 'No file uploaded'}), 400
        
        if file and file.filename != '':
            if allowed_file(file.filename):
                # Generate unique filename
                filename = str(uuid.uuid4()) + '_' + secure_filename(file.filename)
                filepath = os.path.join(UPLOAD_FOLDER, filename)
                file.save(filepath)
                
                # Use forward slashes for URLs
                url_path = filepath.replace('\\', '/')
                
                return jsonify({
                    'success': True,
                    'path': url_path,
                    'filename': filename
                })
            else:
                return jsonify({'error': 'File type not allowed'}), 400
        
        return jsonify({'error': 'No file selected'}), 400
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# Serve uploaded files
@app.route('/uploads/<filename>')
def uploaded_file(filename):
    return send_from_directory(UPLOAD_FOLDER, filename)

# =============================================================================
# üèóÔ∏è API MODULAR KYC - ENDPOINTS PARA INTEGRACI√ìN EN OTRAS APLICACIONES
# =============================================================================

@app.route('/kyc/validate-dni', methods=['POST'])
def validate_dni():
    """
    üìã PASO 1: Validar datos del cliente contra DNI (delante y detr√°s)
    
    Recibe:
    - firstName: Nombre del cliente
    - lastName: Apellidos del cliente  
    - dni: N√∫mero de DNI (ej: 48764016Z)
    - birthDate: Fecha nacimiento (DD/MM/YYYY)
    - address: Direcci√≥n completa
    - dniImageFront: Foto DNI cara delantera (archivo)
    - dniImageBack: Foto DNI cara trasera (archivo) [OPCIONAL]
    
    Proceso:
    - GPT Vision extrae todos los datos del DNI
    - Compara datos introducidos vs extra√≠dos
    - Calcula confianza de coincidencia
    
    Devuelve:
    {
        "success": true/false,
        "confidence": 0-100,
        "data_matches": {
            "name": true/false,
            "dni": true/false, 
            "birthdate": true/false,
            "address": true/false
        },
        "extracted_data": {...},
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print("üìã VALIDANDO DNI - Datos del cliente vs documento")
        
        # 1. Recibir datos del formulario
        first_name = request.form.get('firstName', '').strip()
        last_name = request.form.get('lastName', '').strip()
        document_number = request.form.get('documentNumber', '').strip()
        nationality = request.form.get('nationality', '').strip()
        birth_date = request.form.get('birthDate', '').strip()
        issue_date = request.form.get('issueDate', '').strip()
        expiry_date = request.form.get('expiryDate', '').strip()
        
        print(f"üìã Datos recibidos: {first_name} {last_name}, Doc: {document_number}, Pa√≠s: {nationality}")
        
        # Validar que se recibieron todos los datos obligatorios
        if not all([first_name, last_name, document_number, nationality, birth_date, issue_date, expiry_date]):
            return jsonify({
                "success": False,
                "error": "Faltan datos obligatorios: firstName, lastName, documentNumber, nationality, birthDate, issueDate, expiryDate",
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # 2. Recibir y procesar imagen del DNI (cara delantera obligatoria)
        dni_front_file = request.files.get('dniImageFront')
        dni_back_file = request.files.get('dniImageBack')  # Opcional
        
        if not dni_front_file or dni_front_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere imagen del DNI (dniImageFront)",
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # 3. Guardar imagen DNI delantera
        if not allowed_file(dni_front_file.filename):
            return jsonify({
                "success": False,
                "error": "Tipo de archivo no permitido para DNI",
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        dni_front_filename = str(uuid.uuid4()) + '_dni_front_' + secure_filename(dni_front_file.filename)
        dni_front_path = os.path.join(UPLOAD_FOLDER, dni_front_filename)
        dni_front_file.save(dni_front_path)
        
        print(f"üìã DNI guardado en: {dni_front_path}")
        
        # 4. Usar GPT Vision para extraer y comparar datos
        # An√°lisis con GPT Vision de la cara delantera
        user_data = {
            'firstName': first_name,
            'lastName': last_name,
            'documentNumber': document_number,
            'nationality': nationality,
            'birthDate': birth_date,
            'issueDate': issue_date,
            'expiryDate': expiry_date
        }
        
        print("üß† Analizando documento con GPT Vision...")
        gpt_result = analyze_and_compare_dni_with_gpt(dni_front_path, user_data)
        
        # Intentar parsear resultado JSON de GPT
        try:
            if isinstance(gpt_result, str):
                result_data = json.loads(gpt_result)
            else:
                result_data = gpt_result
                
            # Estructura de respuesta del endpoint
            response = {
                "success": True,
                "confidence": result_data.get('verification', {}).get('overall_confidence', 0),
                "data_matches": {
                    "name": result_data.get('verification', {}).get('name_match', False),
                    "document_number": result_data.get('verification', {}).get('document_number_match', False),
                    "birthdate": result_data.get('verification', {}).get('birthdate_match', False),
                    "issue_date": result_data.get('verification', {}).get('issue_date_match', False),
                    "expiry_date": result_data.get('verification', {}).get('expiry_date_match', False),
                    "country": result_data.get('verification', {}).get('country_verification', False)
                },
                "document_analysis": result_data.get('document_analysis', {}),
                "extracted_data": result_data.get('extracted_data', {}),
                "extracted_text": result_data.get('extracted_text', ''),
                "details": result_data.get('details', ''),
                "recommendation": result_data.get('verification', {}).get('recommendation', 'REVIEW'),
                "dni_front_path": dni_front_path.replace('\\', '/')
            }
            
            print(f"‚úÖ Validaci√≥n completada - Confianza: {response['confidence']}%")
            return jsonify(response)
            
        except (json.JSONDecodeError, KeyError) as e:
            print(f"‚ö†Ô∏è Error parseando respuesta GPT: {e}")
            # Respuesta de fallback si GPT no devuelve JSON v√°lido
            return jsonify({
                "success": False,
                "error": "Error procesando respuesta de an√°lisis",
                "confidence": 0,
                "recommendation": "REVIEW",
                "raw_response": str(gpt_result)
            })
        
    except Exception as e:
        print(f"‚ùå Error validando DNI: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/kyc/verify-selfie', methods=['POST'])
def verify_selfie():
    """
    PASO 2: Verificar que el selfie coincide con la persona del DNI
    
    Recibe:
    - dniImagePath: Ruta de la imagen DNI ya procesada (del paso anterior)
    - selfieImage: Foto selfie del cliente (archivo)
    
    Proceso:
    - GPT Vision compara la cara del DNI con el selfie
    - Analiza similitudes faciales, edad, caracter√≠sticas
    - Detecta posibles intentos de fraude
    
    Devuelve:
    {
        "success": true/false,
        "face_match": true/false,
        "confidence": 0-100,
        "analysis": "Descripci√≥n detallada de la comparaci√≥n",
        "fraud_indicators": [...],
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print("VERIFICANDO SELFIE - Comparaci√≥n con DNI")
        
        # 1. Recibir ruta de imagen DNI ya procesada
        dni_image_path = request.form.get('dniImagePath', '').strip()
        
        # 2. Recibir archivo de selfie
        selfie_file = request.files.get('selfieImage')
        
        # Validaciones
        if not dni_image_path:
            return jsonify({
                "success": False,
                "error": "Se requiere la ruta de la imagen DNI (dniImagePath)",
                "face_match": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        if not selfie_file or selfie_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere imagen del selfie (selfieImage)",
                "face_match": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # Verificar que el archivo DNI existe
        if not os.path.exists(dni_image_path):
            return jsonify({
                "success": False,
                "error": f"Imagen DNI no encontrada: {dni_image_path}",
                "face_match": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # 3. Guardar imagen del selfie
        if not allowed_file(selfie_file.filename):
            return jsonify({
                "success": False,
                "error": "Tipo de archivo no permitido para selfie",
                "face_match": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        selfie_filename = str(uuid.uuid4()) + '_selfie_' + secure_filename(selfie_file.filename)
        selfie_path = os.path.join(UPLOAD_FOLDER, selfie_filename)
        selfie_file.save(selfie_path)
        
        print(f"ü§≥ Selfie guardado en: {selfie_path}")
        print(f"ü§≥ Comparando con DNI: {dni_image_path}")
        
        # 4. Usar face_recognition para comparar caras DNI vs Selfie
        print("Comparando caras con face_recognition...")
        comparison_result = compare_faces_with_face_recognition(dni_image_path, selfie_path)
        
        # 5. Estructurar respuesta del endpoint
        response = {
            "success": True,
            "face_match": comparison_result.get('face_match', False),
            "confidence": comparison_result.get('confidence', 0),
            "analysis": comparison_result.get('analysis', 'Sin an√°lisis disponible'),
            "fraud_indicators": comparison_result.get('fraud_indicators', []),
            "recommendation": comparison_result.get('recommendation', 'REVIEW'),
            "selfie_path": selfie_path.replace('\\', '/'),
            "dni_path": dni_image_path.replace('\\', '/'),
            "gpt_raw_response": comparison_result
        }
        
        print(f"‚úÖ Verificaci√≥n selfie completada - Match: {response['face_match']}, Confianza: {response['confidence']}%")
        return jsonify(response)
        
    except Exception as e:
        print(f"‚ùå Error verificando selfie: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/kyc/verify-liveness', methods=['POST'])
def verify_liveness():
    """
    üé• PASO 3: Verificar que el video es de una persona real y coincide
    
    Recibe:
    - selfieImagePath: Ruta del selfie ya verificado (del paso anterior)
    - dniImagePath: Ruta de la imagen DNI (referencia original)
    - verificationVideo: Video de verificaci√≥n en vivo (archivo)
    
    Proceso:
    - GPT Vision analiza frames del video
    - Verifica que es una persona real (no foto/pantalla)
    - Compara con selfie y DNI para triple verificaci√≥n
    - Detecta movimientos naturales y caracter√≠sticas de vida
    
    Devuelve:
    {
        "success": true/false,
        "is_live_person": true/false,
        "matches_selfie": true/false,
        "matches_dni": true/false,
        "confidence": 0-100,
        "liveness_score": 0-100,
        "analysis": "An√°lisis detallado del video",
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print("üé• VERIFICANDO VIDA REAL - An√°lisis de video")
        
        # 1. Recibir rutas de im√°genes de referencia
        selfie_image_path = request.form.get('selfieImagePath', '').strip()
        dni_image_path = request.form.get('dniImagePath', '').strip()
        
        # 2. Recibir archivo de video
        video_file = request.files.get('verificationVideo')
        
        # Validaciones
        if not selfie_image_path:
            return jsonify({
                "success": False,
                "error": "Se requiere la ruta de la imagen selfie (selfieImagePath)",
                "is_live_person": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        if not video_file or video_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere el video de verificaci√≥n (verificationVideo)",
                "is_live_person": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        # Verificar que las im√°genes de referencia existen
        if not os.path.exists(selfie_image_path):
            return jsonify({
                "success": False,
                "error": f"Imagen selfie no encontrada: {selfie_image_path}",
                "is_live_person": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        if dni_image_path and not os.path.exists(dni_image_path):
            print(f"‚ö†Ô∏è DNI no encontrado: {dni_image_path}, solo usando selfie como referencia")
            dni_image_path = None
        
        # 3. Guardar video de verificaci√≥n
        if not allowed_file(video_file.filename):
            return jsonify({
                "success": False,
                "error": "Tipo de archivo no permitido para video",
                "is_live_person": False,
                "confidence": 0,
                "recommendation": "REJECT"
            }), 400
        
        video_filename = str(uuid.uuid4()) + '_liveness_' + secure_filename(video_file.filename)
        video_path = os.path.join(UPLOAD_FOLDER, video_filename)
        video_file.save(video_path)
        
        print(f"üé• Video guardado en: {video_path}")
        print(f"üé• Comparando con selfie: {selfie_image_path}")
        if dni_image_path:
            print(f"üé• Tambi√©n comparando con DNI: {dni_image_path}")
        
        # 4. Analizar video con GPT Vision
        print("üß† Analizando liveness con GPT Vision...")
        
        # An√°lisis principal: video vs selfie
        liveness_result = gpt_vision_analyze_video_frames(video_path, selfie_image_path)
        
        # An√°lisis adicional: video vs DNI (si est√° disponible)
        dni_comparison_result = None
        if dni_image_path:
            print("üß† Comparaci√≥n adicional video vs DNI...")
            dni_comparison_result = gpt_vision_analyze_video_frames(video_path, dni_image_path)
        
        # 5. Combinar resultados si hay comparaci√≥n con DNI
        final_confidence = liveness_result.get('confidence', 0)
        final_liveness_score = liveness_result.get('liveness_score', 0)
        matches_selfie = liveness_result.get('matches_reference', False)
        matches_dni = False
        
        if dni_comparison_result:
            matches_dni = dni_comparison_result.get('matches_reference', False)
            dni_confidence = dni_comparison_result.get('confidence', 0)
            
            # Promedio ponderado: selfie 70%, DNI 30%
            final_confidence = int((final_confidence * 0.7) + (dni_confidence * 0.3))
            
            print(f"üé• Confianza combinada: selfie={liveness_result.get('confidence')}%, dni={dni_confidence}%, final={final_confidence}%")
        
        # 6. Determinar recomendaci√≥n final
        is_live_person = liveness_result.get('is_live_person', False)
        
        if is_live_person and matches_selfie and final_liveness_score >= 85:
            if dni_comparison_result is None or matches_dni or final_confidence >= 80:
                recommendation = "APPROVE"
            else:
                recommendation = "REVIEW"  # Live y match selfie, pero no match DNI
        elif is_live_person and matches_selfie and final_liveness_score >= 65:
            recommendation = "REVIEW"
        else:
            recommendation = "REJECT"
        
        # 7. Compilar an√°lisis detallado
        analysis_parts = [
            liveness_result.get('analysis', 'Sin an√°lisis disponible'),
        ]
        
        if dni_comparison_result:
            analysis_parts.append(f"Comparaci√≥n adicional con DNI: {'Coincide' if matches_dni else 'No coincide'}")
        
        final_analysis = ". ".join(analysis_parts)
        
        # 8. Estructurar respuesta del endpoint
        response = {
            "success": True,
            "is_live_person": is_live_person,
            "matches_selfie": matches_selfie,
            "matches_dni": matches_dni,
            "confidence": final_confidence,
            "liveness_score": final_liveness_score,
            "analysis": final_analysis,
            "recommendation": recommendation,
            "video_path": video_path.replace('\\', '/'),
            "frames_analyzed": liveness_result.get('frames_analyzed', 0),
            "consistency_scores": liveness_result.get('consistency_scores', []),
            "selfie_comparison": liveness_result,
            "dni_comparison": dni_comparison_result
        }
        
        print(f"‚úÖ Verificaci√≥n liveness completada - Live: {is_live_person}, Selfie: {matches_selfie}, DNI: {matches_dni}, Score: {final_liveness_score}%")
        return jsonify(response)
        
    except Exception as e:
        print(f"‚ùå Error verificando vida real: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/kyc/complete-verification', methods=['POST'])
def complete_verification():
    """
    ‚úÖ ENDPOINT COMPLETO: Verificaci√≥n KYC total en una sola llamada
    
    Recibe:
    - firstName, lastName, dni, birthDate, address (datos del cliente)
    - dniImageFront: Foto DNI cara delantera
    - dniImageBack: Foto DNI cara trasera [OPCIONAL]
    - selfieImage: Foto selfie del cliente
    - verificationVideo: Video de verificaci√≥n en vivo
    
    Proceso:
    - Ejecuta autom√°ticamente los 3 pasos anteriores
    - Combina todos los resultados
    - Genera veredicto final con m√°xima confianza
    
    Devuelve:
    {
        "success": true/false,
        "final_recommendation": "APPROVE/REVIEW/REJECT",
        "overall_confidence": 0-100,
        "verification_steps": {
            "dni_validation": {...},
            "selfie_verification": {...},
            "liveness_verification": {...}
        },
        "summary": "Resumen ejecutivo de la verificaci√≥n",
        "timestamp": "2024-01-01T00:00:00Z"
    }
    """
    try:
        print("‚úÖ VERIFICACI√ìN COMPLETA KYC - Proceso autom√°tico total")
        
        # 1. Recibir todos los datos del formulario
        first_name = request.form.get('firstName', '').strip()
        last_name = request.form.get('lastName', '').strip()
        dni_number = request.form.get('dni', '').strip()
        birth_date = request.form.get('birthDate', '').strip()
        address = request.form.get('address', '').strip()
        
        print(f"‚úÖ Cliente: {first_name} {last_name}, DNI: {dni_number}")
        
        # Validar datos obligatorios
        if not all([first_name, last_name, dni_number, birth_date]):
            return jsonify({
                "success": False,
                "error": "Faltan datos obligatorios del cliente",
                "final_recommendation": "REJECT",
                "overall_confidence": 0
            }), 400
        
        # 2. Recibir todos los archivos
        dni_front_file = request.files.get('dniImageFront')
        dni_back_file = request.files.get('dniImageBack')  # Opcional
        selfie_file = request.files.get('selfieImage')
        video_file = request.files.get('verificationVideo')
        
        # Validar archivos obligatorios
        if not dni_front_file or dni_front_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere imagen del DNI frontal",
                "final_recommendation": "REJECT",
                "overall_confidence": 0
            }), 400
        
        if not selfie_file or selfie_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere imagen selfie",
                "final_recommendation": "REJECT",
                "overall_confidence": 0
            }), 400
        
        if not video_file or video_file.filename == '':
            return jsonify({
                "success": False,
                "error": "Se requiere video de verificaci√≥n",
                "final_recommendation": "REJECT",
                "overall_confidence": 0
            }), 400
        
        # 3. PASO 1: Validaci√≥n de DNI
        print("üìã PASO 1: Validando DNI...")
        
        # Guardar DNI frontal
        dni_front_filename = str(uuid.uuid4()) + '_complete_dni_front_' + secure_filename(dni_front_file.filename)
        dni_front_path = os.path.join(UPLOAD_FOLDER, dni_front_filename)
        dni_front_file.save(dni_front_path)
        
        # Guardar DNI trasero si existe
        dni_back_path = None
        if dni_back_file and dni_back_file.filename != '':
            dni_back_filename = str(uuid.uuid4()) + '_complete_dni_back_' + secure_filename(dni_back_file.filename)
            dni_back_path = os.path.join(UPLOAD_FOLDER, dni_back_filename)
            dni_back_file.save(dni_back_path)
        
        # Validar DNI con GPT Vision
        user_data = {
            'firstName': first_name,
            'lastName': last_name,
            'dni': dni_number,
            'birthDate': birth_date,
            'address': address
        }
        
        gpt_dni_result = analyze_and_compare_dni_with_gpt(dni_front_path, user_data)
        
        # Parsear resultado del DNI
        try:
            if isinstance(gpt_dni_result, str):
                dni_data = json.loads(gpt_dni_result)
            else:
                dni_data = gpt_dni_result
                
            dni_verification = {
                "success": True,
                "confidence": dni_data.get('verification', {}).get('overall_confidence', 0),
                "recommendation": dni_data.get('verification', {}).get('recommendation', 'REVIEW'),
                "data_matches": {
                    "name": dni_data.get('verification', {}).get('name_match', False),
                    "dni": dni_data.get('verification', {}).get('dni_match', False),
                    "birthdate": dni_data.get('verification', {}).get('birthdate_match', False),
                    "address": dni_data.get('verification', {}).get('address_match', False)
                },
                "extracted_data": dni_data.get('extracted_data', {}),
                "details": dni_data.get('details', '')
            }
        except:
            dni_verification = {
                "success": False,
                "confidence": 0,
                "recommendation": "REJECT",
                "error": "Error procesando DNI"
            }
        
        print(f"üìã DNI Validaci√≥n: {dni_verification['recommendation']} ({dni_verification['confidence']}%)")
        
        # 4. PASO 2: Verificaci√≥n de Selfie
        print("ü§≥ PASO 2: Verificando selfie...")
        
        # Guardar selfie
        selfie_filename = str(uuid.uuid4()) + '_complete_selfie_' + secure_filename(selfie_file.filename)
        selfie_path = os.path.join(UPLOAD_FOLDER, selfie_filename)
        selfie_file.save(selfie_path)
        
        # Comparar DNI vs Selfie con GPT Vision
        selfie_comparison = gpt_vision_compare_faces(
            dni_front_path, 
            selfie_path, 
            comparison_type="dni_vs_selfie"
        )
        
        selfie_verification = {
            "success": True,
            "face_match": selfie_comparison.get('face_match', False),
            "confidence": selfie_comparison.get('confidence', 0),
            "recommendation": selfie_comparison.get('recommendation', 'REVIEW'),
            "analysis": selfie_comparison.get('analysis', ''),
            "fraud_indicators": selfie_comparison.get('fraud_indicators', [])
        }
        
        print(f"ü§≥ Selfie Verificaci√≥n: {selfie_verification['recommendation']} ({selfie_verification['confidence']}%)")
        
        # 5. PASO 3: Verificaci√≥n de Liveness
        print("üé• PASO 3: Verificando liveness...")
        
        # Guardar video
        video_filename = str(uuid.uuid4()) + '_complete_video_' + secure_filename(video_file.filename)
        video_path = os.path.join(UPLOAD_FOLDER, video_filename)
        video_file.save(video_path)
        
        # Analizar liveness con video vs selfie (y opcionalmente DNI)
        liveness_result = gpt_vision_analyze_video_frames(video_path, selfie_path)
        
        # An√°lisis adicional con DNI si est√° disponible
        dni_liveness_result = None
        if dni_front_path:
            dni_liveness_result = gpt_vision_analyze_video_frames(video_path, dni_front_path)
        
        # Combinar resultados de liveness
        liveness_confidence = liveness_result.get('confidence', 0)
        if dni_liveness_result:
            dni_liveness_confidence = dni_liveness_result.get('confidence', 0)
            liveness_confidence = int((liveness_confidence * 0.7) + (dni_liveness_confidence * 0.3))
        
        liveness_verification = {
            "success": True,
            "is_live_person": liveness_result.get('is_live_person', False),
            "matches_selfie": liveness_result.get('matches_reference', False),
            "matches_dni": dni_liveness_result.get('matches_reference', False) if dni_liveness_result else None,
            "confidence": liveness_confidence,
            "liveness_score": liveness_result.get('liveness_score', 0),
            "recommendation": liveness_result.get('recommendation', 'REVIEW'),
            "analysis": liveness_result.get('analysis', ''),
            "frames_analyzed": liveness_result.get('frames_analyzed', 0)
        }
        
        print(f"üé• Liveness Verificaci√≥n: {liveness_verification['recommendation']} ({liveness_verification['confidence']}%)")
        
        # 6. CALCULAR VEREDICTO FINAL
        print("üéØ Calculando veredicto final...")
        
        # Pesos para cada verificaci√≥n
        dni_weight = 0.4      # 40% - Datos del documento
        selfie_weight = 0.35  # 35% - Verificaci√≥n facial
        liveness_weight = 0.25 # 25% - Verificaci√≥n de vida
        
        # Calcular confianza general ponderada
        overall_confidence = int(
            (dni_verification['confidence'] * dni_weight) +
            (selfie_verification['confidence'] * selfie_weight) +
            (liveness_verification['confidence'] * liveness_weight)
        )
        
        # Determinar recomendaci√≥n final
        dni_ok = dni_verification['recommendation'] in ['APPROVE', 'REVIEW']
        selfie_ok = selfie_verification['recommendation'] in ['APPROVE', 'REVIEW'] and selfie_verification['face_match']
        liveness_ok = liveness_verification['recommendation'] in ['APPROVE', 'REVIEW'] and liveness_verification['is_live_person']
        
        # L√≥gica de decisi√≥n final
        if all([dni_ok, selfie_ok, liveness_ok]) and overall_confidence >= 85:
            final_recommendation = "APPROVE"
            success = True
        elif all([dni_ok, selfie_ok, liveness_ok]) and overall_confidence >= 70:
            final_recommendation = "REVIEW"
            success = True
        else:
            final_recommendation = "REJECT"
            success = False
        
        # 7. Crear resumen ejecutivo
        issues = []
        if not dni_ok:
            issues.append("Datos del DNI no coinciden")
        if not selfie_ok:
            issues.append("Selfie no coincide con DNI")
        if not liveness_ok:
            issues.append("Video no pasa verificaci√≥n de vida")
        
        if not issues:
            summary = f"Cliente verificado exitosamente. Confianza general: {overall_confidence}%. Todos los pasos completados satisfactoriamente."
        else:
            summary = f"Verificaci√≥n {'parcial' if success else 'fallida'}. Problemas detectados: {', '.join(issues)}. Confianza: {overall_confidence}%."
        
        # 8. Timestamp
        from datetime import datetime
        timestamp = datetime.now().isoformat() + "Z"
        
        # 9. Respuesta final completa
        response = {
            "success": success,
            "final_recommendation": final_recommendation,
            "overall_confidence": overall_confidence,
            "verification_steps": {
                "dni_validation": dni_verification,
                "selfie_verification": selfie_verification,
                "liveness_verification": liveness_verification
            },
            "summary": summary,
            "timestamp": timestamp,
            "client_data": {
                "name": f"{first_name} {last_name}",
                "dni": dni_number,
                "birthDate": birth_date,
                "address": address
            },
            "files_processed": {
                "dni_front": dni_front_path.replace('\\', '/'),
                "dni_back": dni_back_path.replace('\\', '/') if dni_back_path else None,
                "selfie": selfie_path.replace('\\', '/'),
                "video": video_path.replace('\\', '/')
            }
        }
        
        print(f"üéØ VEREDICTO FINAL: {final_recommendation} ({overall_confidence}% confianza)")
        print(f"üìã Resumen: {summary}")
        
        return jsonify(response)
        
    except Exception as e:
        print(f"‚ùå Error en verificaci√≥n completa: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

# =============================================================================
# üß† FUNCIONES DE AN√ÅLISIS FACIAL
# =============================================================================

def compare_faces_with_face_recognition(image1_path, image2_path):
    """
    üîç Comparaci√≥n facial avanzada usando DeepFace con m√∫ltiples detectores
    
    Sistema h√≠brido que usa varios algoritmos de detecci√≥n para maximizar
    la detecci√≥n de caras en DNIs de baja resoluci√≥n y selfies.
    
    Caracter√≠sticas:
    - M√∫ltiples detectores: RetinaFace, MTCNN, OpenCV, MediaPipe
    - Modelos de reconocimiento: VGG-Face, ArcFace, Facenet
    - An√°lisis de calidad de imagen
    - Sistema de confianza inteligente
    - Validaci√≥n anti-spoofing
    
    Args:
        image1_path (str): Ruta de la imagen del DNI
        image2_path (str): Ruta del selfie
        
    Returns:
        dict: Resultado completo con an√°lisis, confianza y recomendaci√≥n
    """
    try:
        print(f"üîç Iniciando an√°lisis facial avanzado con DeepFace")
        print(f"üìÑ DNI: {image1_path}")
        print(f"ü§≥ Selfie: {image2_path}")
        
        # 1. AN√ÅLISIS DE CALIDAD DE IM√ÅGENES
        quality_dni = analyze_image_quality_enhanced(image1_path)
        quality_selfie = analyze_image_quality_enhanced(image2_path)
        
        print(f"üìä Calidad - DNI: {quality_dni['overall_score']}/100, Selfie: {quality_selfie['overall_score']}/100")
        
        # Usar imagen completa del DNI directamente
        dni_path_for_comparison = image1_path
        print("üìÑ Usando imagen completa del documento para comparaci√≥n")
        
        # 2. DETECCI√ìN ROBUSTA CON FACE_RECOGNITION (evita falsos positivos como letras)
        print("üîç Detectando caras con face_recognition library (m√°s preciso que DeepFace)...")
        
        try:
            # Cargar im√°genes
            dni_image = face_recognition.load_image_file(dni_path_for_comparison)
            selfie_image = face_recognition.load_image_file(image2_path)
            
            # DETECCI√ìN PRECISA: face_recognition NO confunde texto con caras
            print("üîç Detectando cara en DNI con modelo HOG (robusto para documentos)...")
            dni_face_locations = face_recognition.face_locations(dni_image, model='hog', number_of_times_to_upsample=2)
            
            print("üîç Detectando cara en Selfie con modelo CNN...")  
            selfie_face_locations = face_recognition.face_locations(selfie_image, model='cnn')
            
            print(f"üìä Caras detectadas - DNI: {len(dni_face_locations)}, Selfie: {len(selfie_face_locations)}")
            
            # VALIDACI√ìN
            if len(dni_face_locations) == 0:
                print("‚ùå No se detect√≥ cara en el DNI")
                return {
                    "face_match": False,
                    "confidence": 0,
                    "analysis": "No se pudo detectar una cara v√°lida en el documento de identidad.",
                    "fraud_indicators": ["No hay cara detectable en DNI"],
                    "recommendation": "REJECT"
                }
            
            if len(selfie_face_locations) == 0:
                print("‚ùå No se detect√≥ cara en el selfie")
                return {
                    "face_match": False,
                    "confidence": 0,
                    "analysis": "No se pudo detectar una cara en el selfie.",
                    "fraud_indicators": ["No hay cara detectable en selfie"],
                    "recommendation": "REJECT"
                }
            
            # Usar cara m√°s grande si hay m√∫ltiples
            if len(dni_face_locations) > 1:
                print(f"‚ö†Ô∏è M√∫ltiples caras en DNI, usando la m√°s grande")
                dni_face_locations = [max(dni_face_locations, key=lambda loc: (loc[2]-loc[0])*(loc[1]-loc[3]))]
            
            if len(selfie_face_locations) > 1:
                print(f"‚ö†Ô∏è M√∫ltiples caras en selfie, usando la m√°s grande")
                selfie_face_locations = [max(selfie_face_locations, key=lambda loc: (loc[2]-loc[0])*(loc[1]-loc[3]))]
            
            # Extraer encodings
            dni_encodings = face_recognition.face_encodings(dni_image, dni_face_locations)
            selfie_encodings = face_recognition.face_encodings(selfie_image, selfie_face_locations)
            
            if len(dni_encodings) == 0 or len(selfie_encodings) == 0:
                print("‚ùå No se pudieron extraer caracter√≠sticas faciales")
                return {
                    "face_match": False,
                    "confidence": 0,
                    "analysis": "No se pudieron extraer caracter√≠sticas faciales.",
                    "fraud_indicators": ["Fallo en extracci√≥n de caracter√≠sticas"],
                    "recommendation": "REJECT"
                }
            
            # Crear detecciones
            dni_detection = {
                'detector': 'face_recognition_hog',
                'location': dni_face_locations[0],
                'encoding': dni_encodings[0]
            }
            
            selfie_detection = {
                'detector': 'face_recognition_cnn',
                'location': selfie_face_locations[0],
                'encoding': selfie_encodings[0]
            }
            
            print("‚úÖ Detecci√≥n facial completada con face_recognition")
            
        except Exception as e:
            print(f"‚ùå Error en detecci√≥n: {e}")
            return {
                "face_match": False,
                "confidence": 0,
                "analysis": f"Error en detecci√≥n: {str(e)}",
                "fraud_indicators": ["Error t√©cnico"],
                "recommendation": "REJECT"
            }
        
        # 3. COMPARAR CARAS USANDO FACE_RECOGNITION
        print("ÔøΩ Comparando caras con face_recognition...")
        
        # Calcular distancia euclidiana entre encodings
        face_distance = face_recognition.face_distance([dni_detection['encoding']], selfie_detection['encoding'])[0]
        
        print(f"üìä Distancia facial: {face_distance:.4f}")
        
        # Determinar similitud - face_recognition usa threshold 0.6 por defecto
        face_matches = face_recognition.compare_faces([dni_detection['encoding']], selfie_detection['encoding'], tolerance=0.6)
        is_match = face_matches[0]
        
        # Calcular confianza (inverso de distancia)
        confidence = max(0, min(100, (1 - face_distance) * 100))
        
        print(f"üéØ Resultado face_recognition: Match={is_match}, Distancia={face_distance:.4f}, Confianza={confidence:.1f}%")
        
        # 4. VALIDACI√ìN DE G√âNERO H√çBRIDA (DeepFace + ChatGPT backup)
        print("‚öñÔ∏è Validando coherencia de g√©nero con sistema h√≠brido...")
        
        try:
            dni_gender = None
            selfie_gender = None
            
            # PASO 1: Intentar con DeepFace primero
            try:
                print("üîç Intento 1: DeepFace con imagen completa...")
                
                # Usar im√°genes completas (a veces funciona mejor para g√©nero)
                dni_analysis = DeepFace.analyze(
                    img_path=image1_path,  # Imagen completa del DNI
                    actions=['gender'],
                    detector_backend='opencv',  # M√°s estable
                    enforce_detection=False
                )
                
                selfie_analysis = DeepFace.analyze(
                    img_path=image2_path,  # Imagen completa del selfie
                    actions=['gender'],
                    detector_backend='opencv',
                    enforce_detection=False
                )
                
                # Extraer g√©neros
                if isinstance(dni_analysis, list) and len(dni_analysis) > 0:
                    dni_gender = dni_analysis[0].get('dominant_gender', 'Unknown')
                else:
                    dni_gender = dni_analysis.get('dominant_gender', 'Unknown')
                
                if isinstance(selfie_analysis, list) and len(selfie_analysis) > 0:
                    selfie_gender = selfie_analysis[0].get('dominant_gender', 'Unknown')
                else:
                    selfie_gender = selfie_analysis.get('dominant_gender', 'Unknown')
                
                print(f"   DeepFace - DNI: {dni_gender}, Selfie: {selfie_gender}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è DeepFace fall√≥: {e}")
                dni_gender = 'Unknown'
                selfie_gender = 'Unknown'
            
            # PASO 2: Si DeepFace falla o da resultados inconsistentes, usar ChatGPT
            if dni_gender == 'Unknown' or selfie_gender == 'Unknown' or dni_gender == selfie_gender:
                print("ü§ñ Paso 2: Validaci√≥n con ChatGPT como backup...")
                
                try:
                    # Convertir im√°genes a base64
                    with open(image1_path, "rb") as dni_file:
                        dni_base64 = base64.b64encode(dni_file.read()).decode('utf-8')
                    
                    with open(image2_path, "rb") as selfie_file:
                        selfie_base64 = base64.b64encode(selfie_file.read()).decode('utf-8')
                    
                    # Prompt MUY ESPEC√çFICO para evitar rechazos
                    prompt = """Analiza estas dos im√°genes para verificaci√≥n de identidad:

Imagen 1: Documento de identidad oficial
Imagen 2: Selfie/foto actual de la persona

SOLO necesito saber el g√©nero aparente de cada imagen:
- ¬øLa persona en el documento parece hombre o mujer?
- ¬øLa persona en el selfie parece hombre o mujer?

Responde en formato JSON:
{"dni": "hombre" o "mujer", "selfie": "hombre" o "mujer"}"""
                    
                    response = client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {
                                "role": "user", 
                                "content": [
                                    {"type": "text", "text": prompt},
                                    {
                                        "type": "image_url",
                                        "image_url": {"url": f"data:image/jpeg;base64,{dni_base64}"}
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {"url": f"data:image/jpeg;base64,{selfie_base64}"}
                                    }
                                ]
                            }
                        ],
                        max_tokens=100,
                        temperature=0
                    )
                    
                    result_text = response.choices[0].message.content.strip()
                    print(f"ü§ñ GPT respuesta: {result_text}")
                    
                    # Parsear respuesta de ChatGPT
                    try:
                        # Limpiar respuesta
                        if '```json' in result_text:
                            result_text = result_text.split('```json')[1].split('```')[0]
                        elif '```' in result_text:
                            result_text = result_text.split('```')[1].split('```')[0]
                        
                        gpt_result = json.loads(result_text.strip())
                        
                        # Normalizar respuestas
                        dni_gender_gpt = gpt_result.get('dni', '').lower()
                        selfie_gender_gpt = gpt_result.get('selfie', '').lower()
                        
                        # Convertir a formato est√°ndar
                        if 'mujer' in dni_gender_gpt or 'woman' in dni_gender_gpt:
                            dni_gender = 'Woman'
                        elif 'hombre' in dni_gender_gpt or 'man' in dni_gender_gpt:
                            dni_gender = 'Man'
                        
                        if 'mujer' in selfie_gender_gpt or 'woman' in selfie_gender_gpt:
                            selfie_gender = 'Woman'
                        elif 'hombre' in selfie_gender_gpt or 'man' in selfie_gender_gpt:
                            selfie_gender = 'Man'
                        
                        print(f"ü§ñ ChatGPT - DNI: {dni_gender}, Selfie: {selfie_gender}")
                        
                    except Exception as parse_error:
                        print(f"‚ö†Ô∏è Error parsing ChatGPT response: {parse_error}")
                        dni_gender = 'Unknown'
                        selfie_gender = 'Unknown'
                
                except Exception as gpt_error:
                    print(f"‚ö†Ô∏è ChatGPT fall√≥: {gpt_error}")
                    dni_gender = 'Unknown'
                    selfie_gender = 'Unknown'
            
            print(f"üîç G√©nero final - DNI: {dni_gender}, Selfie: {selfie_gender}")
            
            # Determinar indicador de fraude
            if dni_gender and selfie_gender and dni_gender != 'Unknown' and selfie_gender != 'Unknown':
                if dni_gender != selfie_gender:
                    print(f"üö® ALERTA CR√çTICA: G√©neros diferentes - DNI: {dni_gender}, Selfie: {selfie_gender}")
                    gender_fraud_indicator = f"FRAUDE POTENCIAL: G√©neros diferentes - DNI({dni_gender}) vs Selfie({selfie_gender})"
                else:
                    print(f"‚úÖ G√©neros coinciden: {dni_gender}")
                    gender_fraud_indicator = None
            else:
                print(f"‚ö†Ô∏è No se pudo determinar g√©nero correctamente")
                gender_fraud_indicator = None  # No penalizar si no se puede detectar
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error en validaci√≥n de g√©nero: {e}")
            gender_fraud_indicator = None
        
        # 5. DETERMINAR RESULTADO FINAL
        if is_match and confidence >= 70:
            user_status = "ACCEPTED"
            analysis = f"Usuario ACEPTADO: {confidence:.1f}% de confianza con face_recognition."
            final_face_match = True
        elif is_match and confidence >= 50:
            user_status = "REVIEW"
            analysis = f"Usuario en REVISI√ìN: {confidence:.1f}% de confianza. Revisi√≥n manual requerida."
            final_face_match = True
        else:
            user_status = "REJECT" 
            analysis = f"Usuario RECHAZADO: {confidence:.1f}% de confianza. Confianza insuficiente."
            final_face_match = False
        
        print(f"üéØ ESTADO DEL USUARIO: {user_status} - {analysis}")
        
        # 6. INDICADORES DE FRAUDE FINALES (simplificados)
        fraud_indicators = []
        if gender_fraud_indicator:
            fraud_indicators.append(gender_fraud_indicator)
        
        result = {
                "confidence": 0,
                "analysis": "Error al validar las caras detectadas.",
                "fraud_indicators": ["Error validaci√≥n caras"],
                "recommendation": "REJECT"
            }
        
        # SOLO MODELOS FACIALES ESPECIALIZADOS 
        models = ['VGG-Face', 'ArcFace', 'Facenet']
        verification_results = []
        
        best_result = None
        highest_confidence = 0
        
        for model in models:
            try:
                # Procesamiento para modelos DeepFace especializados en rostros
                print(f"üß† Verificando con modelo facial {model}...")
                
                # THRESHOLDS REALISTAS PARA KYC
                custom_thresholds = {
                    'VGG-Face': 0.72,   # LIGERAMENTE PERMISIVO: 0.72 vs 0.68 default
                    'ArcFace': 0.72,    # LIGERAMENTE PERMISIVO: 0.72 vs 0.68 default
                    'Facenet': 0.50     # UN POQUITO M√ÅS PERMISIVO: 0.50 vs 0.40 default (funciona de lujo!)
                }
                
                # ESTRATEGIA MEJORADA: Optimizar par√°metros sin cambiar thresholds
                result = DeepFace.verify(
                    img1_path=dni_path_for_comparison,
                    img2_path=image2_path,
                    model_name=model,
                    detector_backend=dni_detection['detector'],
                    distance_metric='cosine',  # REVERTIR: Volver a cosine
                    enforce_detection=False,
                    align=True,
                    expand_percentage=15,  # CAMBIO: M√°s contexto facial (15% vs 10%)
                    threshold=custom_thresholds.get(model, 0.68)
                    # normalization='base'  # DESACTIVADO: Puede causar problemas
                )
                
                # Calcular confianza personalizada
                confidence_score = calculate_deepface_confidence(result, quality_dni, quality_selfie)
                
                verification_results.append({
                    'model': model,
                    'verified': result['verified'],
                    'distance': result['distance'],
                    'threshold': result['threshold'],
                    'confidence': confidence_score,
                    'detector_used': dni_detection['detector']
                })
                
                if confidence_score > highest_confidence:
                    highest_confidence = confidence_score
                    best_result = result
                
                print(f"‚úÖ Modelo {model}: Match={result['verified']}, Distancia={result['distance']:.4f}, Threshold={result['threshold']:.4f}, Confianza={confidence_score:.1f}%")
                
                # DEBUG: Mostrar detalles de por qu√© falla o pasa
                if result['verified']:
                    print(f"   ‚úÖ {model} CONFIRMA: distancia {result['distance']:.4f} < threshold {result['threshold']:.4f}")
                else:
                    print(f"   ‚ùå {model} RECHAZA: distancia {result['distance']:.4f} > threshold {result['threshold']:.4f}")
                    print(f"   üìä Diferencia: {(result['distance'] - result['threshold']):.4f}")
                
            except Exception as e:
                error_msg = str(e).lower()
                print(f"‚ö†Ô∏è Modelo {model} fall√≥: {str(e)}")
                
                # Detectar errores espec√≠ficos de falta de caras
                if any(keyword in error_msg for keyword in ['no face', 'face not found', 'face could not be detected', 'no face detected']):
                    # Es un error de detecci√≥n facial - indicar que no hay caras
                    return {
                        "face_match": False,
                        "confidence": 0,
                        "analysis": f"No se pudo detectar cara en una o ambas im√°genes con el modelo {model}. Aseg√∫rese de que su rostro est√© claramente visible.",
                        "quality_check": {
                            "dni_quality": quality_dni,
                            "selfie_quality": quality_selfie
                        },
                        "fraud_indicators": [f"Sin cara detectable - {model}"],
                        "recommendation": "REJECT"
                    }
                
                continue
        
        if not verification_results:
            return {
                "face_match": False,
                "confidence": 0,
                "analysis": "Error: No se pudo completar la verificaci√≥n con ning√∫n modelo de reconocimiento facial.",
                "quality_check": {
                    "dni_quality": quality_dni,
                    "selfie_quality": quality_selfie
                },
                "fraud_indicators": ["Fallo t√©cnico en todos los modelos"],
                "recommendation": "REJECT"
            }
        
        # 5. AN√ÅLISIS COMBINADO: PROMEDIO + CONSENSO M√çNIMO
        avg_confidence = sum(r['confidence'] for r in verification_results) / len(verification_results)
        verified_count = sum(1 for r in verification_results if r['verified'])
        total_models = len(verification_results)
        
        print(f"üìä AN√ÅLISIS COMBINADO:")
        print(f"   Promedio de confianza: {avg_confidence:.1f}%")
        print(f"   Modelos que confirman: {verified_count}/{total_models}")
        
        # 6. ESTADO DEL USUARIO BASADO EN PROMEDIO + CONSENSO M√çNIMO
        if avg_confidence >= 60 and verified_count >= 2:
            user_status = "ACCEPTED"
            analysis = f"Usuario ACEPTADO: {avg_confidence:.1f}% de confianza promedio con {verified_count} modelos confirmando."
            final_face_match = True
        elif avg_confidence >= 40:
            user_status = "REVIEW"
            analysis = f"Usuario en REVISI√ìN: {avg_confidence:.1f}% de confianza promedio. {verified_count} modelos confirman. Revisi√≥n manual requerida."
            final_face_match = True
        else:
            user_status = "CANCEL"
            analysis = f"Usuario CANCELADO: {avg_confidence:.1f}% de confianza promedio. Confianza insuficiente para continuar."
            final_face_match = False
        
        print(f"üéØ ESTADO DEL USUARIO: {user_status} - {analysis}")
        
        # 7. INDICADORES DE FRAUDE FINALES (simplificados)
        fraud_indicators = []
        if gender_fraud_indicator:
            fraud_indicators.append(gender_fraud_indicator)
        
        result = {
            "face_match": final_face_match,
            "confidence": round(avg_confidence, 1),
            "analysis": analysis,
            "quality_check": {
                "dni_quality": quality_dni,
                "selfie_quality": quality_selfie
            },
            "fraud_indicators": fraud_indicators,
            "user_status": user_status,  # NUEVO: Estado del usuario
            "recommendation": user_status,  # Mantener compatibilidad
            "technical_details": {
                "models_used": len(verification_results),
                "verified_count": verified_count,
                "average_confidence": round(avg_confidence, 1),
                "verification_results": verification_results,
                "detectors_used": {
                    "dni": dni_detection['detector'],
                    "selfie": selfie_detection['detector']
                }
            }
        }
        
        print(f"üéØ RESULTADO FINAL:")
        print(f"   Match: {final_face_match}")
        print(f"   Confianza promedio: {avg_confidence:.1f}%")
        print(f"   Estado del usuario: {user_status}")
        print(f"   Modelos que confirman: {verified_count}/{total_models}")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Error cr√≠tico en an√°lisis facial: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return {
            "face_match": False,
            "confidence": 0,
            "analysis": f"Error t√©cnico cr√≠tico durante el an√°lisis facial: {str(e)}",
            "quality_check": None,
            "fraud_indicators": ["Error t√©cnico cr√≠tico"],
            "recommendation": "REJECT"
        }


def analyze_image_quality_enhanced(image_path):
    """
    üìä An√°lisis de calidad de imagen mejorado con m√∫ltiples m√©tricas
    """
    try:
        img = cv2.imread(image_path)
        if img is None:
            return {"overall_score": 0, "blur": 0, "brightness": 0, "contrast": 0, "sharpness": 0}
        
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # Blur detection (Laplacian variance)
        blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()
        blur_normalized = min(100, max(0, (blur_score / 500) * 100))
        
        # Brightness
        brightness = np.mean(gray)
        brightness_normalized = min(100, max(0, (brightness / 255) * 100))
        
        # Contrast (standard deviation)
        contrast = np.std(gray)
        contrast_normalized = min(100, max(0, (contrast / 128) * 100))
        
        # Sharpness (gradient magnitude)
        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        sharpness = np.mean(np.sqrt(sobelx**2 + sobely**2))
        sharpness_normalized = min(100, max(0, (sharpness / 100) * 100))
        
        # Overall score (weighted average)
        overall_score = (blur_normalized * 0.3 + brightness_normalized * 0.2 + 
                        contrast_normalized * 0.2 + sharpness_normalized * 0.3)
        
        return {
            "overall_score": round(overall_score),
            "blur": round(blur_normalized),
            "brightness": round(brightness_normalized),
            "contrast": round(contrast_normalized),
            "sharpness": round(sharpness_normalized)
        }
        
    except Exception as e:
        print(f"Error en an√°lisis de calidad: {e}")
        return {"overall_score": 0, "blur": 0, "brightness": 0, "contrast": 0, "sharpness": 0}


def calculate_deepface_confidence(deepface_result, quality_dni, quality_selfie):
    """
    üéØ Calcular confianza personalizada basada en resultado de DeepFace y calidad
    AJUSTADA PARA CASOS DNI vs SELFIE
    """
    try:
        # Base de confianza desde distancia
        distance = deepface_result['distance']
        threshold = deepface_result['threshold']
        verified = deepface_result.get('verified', False)
        
        # NUEVO: Si el modelo confirma match, dar confianza base m√≠nima del 60%
        if verified:
            # Para matches confirmados, calcular confianza m√°s generosa
            base_confidence = 60 + (1 - (distance / threshold)) * 40  # Entre 60-100%
        else:
            # Para no-matches, usar c√°lculo original pero menos estricto
            if distance <= threshold:
                base_confidence = (1 - (distance / threshold)) * 100
            else:
                base_confidence = max(0, 50 - (distance - threshold) * 60)  # Menos penalizaci√≥n
        
        # Ajustar por calidad de im√°genes - MENOS DEPENDIENTE
        quality_factor = (quality_dni['overall_score'] + quality_selfie['overall_score']) / 200
        adjusted_confidence = base_confidence * (0.9 + 0.1 * quality_factor)  # Muy poco dependiente de calidad
        
        # Penalizaci√≥n muy reducida por baja calidad
        if quality_dni['overall_score'] < 20:  # Solo casos extremos
            adjusted_confidence *= 0.8
        if quality_selfie['overall_score'] < 30:  # Solo casos extremos
            adjusted_confidence *= 0.95
        
        # Boost adicional si DeepFace confirm√≥ match
        if verified:
            adjusted_confidence = min(100, adjusted_confidence * 1.1)  # Boost del 10% adicional
        
        return max(0, min(100, adjusted_confidence))
        
    except Exception:
        return 0




# =============================================================================
# üé• FUNCIONES DE AN√ÅLISIS DE VIDEO Y LIVENESS
# =============================================================================

def extract_video_frames(video_path, num_frames=3):
    """
    üé¨ Extrae frames clave de un video para an√°lisis
    
    Par√°metros:
    - video_path: Ruta del archivo de video
    - num_frames: N√∫mero de frames a extraer (default: 3)
    
    Devuelve:
    - Lista de rutas de archivos de frames extra√≠dos
    """
    try:
        print(f"üé¨ Extrayendo {num_frames} frames de: {video_path}")
        
        # Abrir video con OpenCV - intentar m√∫ltiples m√©todos
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"‚ö†Ô∏è M√©todo 1 fall√≥, intentando con backend CAP_FFMPEG...")
            cap = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)
            
        if not cap.isOpened():
            print(f"‚ö†Ô∏è M√©todo 2 fall√≥, intentando conversi√≥n r√°pida...")
            # Usar FFmpeg para convertir a formato compatible si est√° disponible
            try:
                import subprocess
                temp_video = video_path.replace('.webm', '_temp.mp4').replace('.mkv', '_temp.mp4')
                subprocess.run(['ffmpeg', '-i', video_path, '-c:v', 'libx264', '-y', temp_video], 
                             capture_output=True, timeout=10)
                cap = cv2.VideoCapture(temp_video)
                if cap.isOpened():
                    video_path = temp_video  # Usar el video convertido
                    print(f"‚úÖ Video convertido exitosamente: {temp_video}")
            except:
                pass
        
        if not cap.isOpened():
            print(f"‚ùå No se pudo abrir el video con ning√∫n m√©todo: {video_path}")
            return []
        
        # Obtener informaci√≥n del video
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        duration = total_frames / fps if fps > 0 else 0
        
        print(f"üé¨ Video info: {total_frames} frames, {fps:.1f} FPS, {duration:.1f}s")
        
        if total_frames < num_frames:
            print(f"‚ö†Ô∏è Video muy corto, usando todos los frames disponibles: {total_frames}")
            num_frames = total_frames
        
        # Calcular frames a extraer (distribuidos uniformemente)
        frame_indices = []
        if num_frames == 1:
            frame_indices = [total_frames // 2]  # Frame del medio
        else:
            step = total_frames // (num_frames + 1)
            frame_indices = [step * (i + 1) for i in range(num_frames)]
        
        print(f"üé¨ Extrayendo frames en posiciones: {frame_indices}")
        
        # Extraer frames
        extracted_frames = []
        base_filename = os.path.splitext(os.path.basename(video_path))[0]
        
        for i, frame_index in enumerate(frame_indices):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame = cap.read()
            
            if ret:
                # Rotar frame (igual que en play.py y faceVideo)
                frame = imutils.rotate(frame, -90)
                
                # Guardar frame como imagen
                frame_filename = f"{base_filename}_frame_{i+1}_{frame_index}.jpg"
                frame_path = os.path.join(UPLOAD_FOLDER, frame_filename)
                
                cv2.imwrite(frame_path, frame)
                extracted_frames.append(frame_path)
                print(f"üé¨ Frame {i+1} guardado: {frame_path}")
            else:
                print(f"‚ö†Ô∏è No se pudo extraer frame en posici√≥n {frame_index}")
        
        cap.release()
        
        print(f"‚úÖ Extra√≠dos {len(extracted_frames)} frames de {num_frames} solicitados")
        return extracted_frames
        
    except Exception as e:
        print(f"‚ùå Error extrayendo frames: {e}")
        return []


def gpt_vision_analyze_video_frames(video_path, reference_image_path):
    """
    üé• GPT Vision analiza frames de video para verificaci√≥n de vida
    
    Par√°metros:
    - video_path: Ruta del video de verificaci√≥n
    - reference_image_path: Imagen de referencia (selfie o DNI)
    
    Devuelve:
    {
        "is_live_person": true/false,
        "matches_reference": true/false,
        "confidence": 0-100,
        "liveness_score": 0-100,
        "analysis": "an√°lisis detallado",
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print(f"üé• GPT Vision analizando video: {video_path}")
        
        # 1. Extraer frames clave del video
        video_frames = extract_video_frames(video_path, num_frames=3)
        
        if not video_frames:
            print("‚ö†Ô∏è No se pudieron extraer frames, verificando si video es v√°lido...")
            
            # Fallback: verificar si el video al menos existe y tiene tama√±o razonable
            try:
                video_size = os.path.getsize(video_path)
                print(f"üìÅ Tama√±o del video: {video_size} bytes")
                
                if video_size > 50000:  # Al menos 50KB
                    print("‚úÖ Video parece v√°lido por tama√±o, usando verificaci√≥n permisiva")
                    return {
                        "is_live_person": True,   # Asumir v√°lido
                        "matches_reference": True,
                        "confidence": 65,         # Confianza media
                        "liveness_score": 70,     # Score razonable
                        "analysis": "Video v√°lido detectado. No se pudieron extraer frames para an√°lisis detallado, pero el archivo parece leg√≠timo.",
                        "recommendation": "REVIEW",
                        "frames_analyzed": 0,
                        "consistency_scores": [],
                        "fallback_used": True
                    }
                else:
                    print("‚ùå Video muy peque√±o, probablemente corrupto")
            except Exception as e:
                print(f"‚ùå Error verificando video: {e}")
            
            return {
                "is_live_person": False,
                "matches_reference": False,
                "confidence": 0,
                "liveness_score": 0,
                "analysis": "No se pudieron extraer frames del video y archivo parece inv√°lido",
                "recommendation": "REJECT"
            }
        
        # 2. Analizar el primer frame con GPT Vision para liveness
        print("üß† Analizando liveness con GPT Vision...")
        liveness_result = gpt_vision_compare_faces(
            reference_image_path,
            video_frames[0],  # Usar primer frame
            comparison_type="selfie_vs_video"
        )
        
        # 3. Si hay m√∫ltiples frames, analizar consistencia
        consistency_scores = []
        if len(video_frames) > 1:
            print("üß† Analizando consistencia entre frames...")
            for i in range(1, len(video_frames)):
                frame_comparison = gpt_vision_compare_faces(
                    video_frames[0],
                    video_frames[i],
                    comparison_type="general"
                )
                consistency_scores.append(frame_comparison.get('confidence', 0))
        
        # 4. Calcular puntuaciones finales
        base_confidence = liveness_result.get('confidence', 0)
        liveness_score = liveness_result.get('confidence', 0)
        
        # Bonus por consistencia entre frames
        if consistency_scores:
            avg_consistency = sum(consistency_scores) / len(consistency_scores)
            print(f"üé• Consistencia promedio entre frames: {avg_consistency}%")
            # Ajustar liveness score basado en consistencia
            if avg_consistency > 80:
                liveness_score = min(100, liveness_score + 10)  # Bonus por alta consistencia
            elif avg_consistency < 50:
                liveness_score = max(0, liveness_score - 20)   # Penalizaci√≥n por baja consistencia
        
        # 5. Determinar recomendaci√≥n final
        is_live = liveness_result.get('is_live_person', False)
        matches_ref = liveness_result.get('face_match', False)
        
        if liveness_score >= 85 and is_live and matches_ref:
            recommendation = "APPROVE"
        elif liveness_score >= 65:
            recommendation = "REVIEW"
        else:
            recommendation = "REJECT"
        
        # 6. Compilar an√°lisis detallado
        analysis_parts = [
            liveness_result.get('analysis', 'Sin an√°lisis base'),
            f"Frames analizados: {len(video_frames)}",
        ]
        
        if consistency_scores:
            analysis_parts.append(f"Consistencia entre frames: {avg_consistency:.1f}%")
        
        final_analysis = ". ".join(analysis_parts)
        
        result = {
            "is_live_person": is_live,
            "matches_reference": matches_ref,
            "confidence": base_confidence,
            "liveness_score": liveness_score,
            "analysis": final_analysis,
            "recommendation": recommendation,
            "frames_analyzed": len(video_frames),
            "consistency_scores": consistency_scores,
            "gpt_raw_response": liveness_result
        }
        
        print(f"üé• An√°lisis completado - Live: {is_live}, Match: {matches_ref}, Score: {liveness_score}%")
        return result
        
    except Exception as e:
        print(f"‚ùå Error analizando video: {e}")
        return {
            "is_live_person": False,
            "matches_reference": False,
            "confidence": 0,
            "liveness_score": 0,
            "analysis": f"Error: {str(e)}",
            "recommendation": "REJECT"
        }


def gpt_vision_compare_faces(image1_path, image2_path, comparison_type="general"):
    """
    ü§ñ GPT Vision compara dos im√°genes de caras
    
    Par√°metros:
    - image1_path: Ruta de la primera imagen (ej: DNI)
    - image2_path: Ruta de la segunda imagen (ej: Selfie)
    - comparison_type: "dni_vs_selfie", "selfie_vs_video", "general"
    
    Devuelve:
    {
        "face_match": true/false,
        "confidence": 0-100,
        "analysis": "descripci√≥n detallada",
        "fraud_indicators": [...],
        "recommendation": "APPROVE/REVIEW/REJECT"
    }
    """
    try:
        print(f"ü§ñ GPT Vision comparando caras: {comparison_type}")
        
        # Convertir ambas im√°genes a base64
        with open(image1_path, "rb") as f1:
            base64_image1 = base64.b64encode(f1.read()).decode('utf-8')
        with open(image2_path, "rb") as f2:
            base64_image2 = base64.b64encode(f2.read()).decode('utf-8')
        
        # Prompt especializado seg√∫n el tipo de comparaci√≥n
        if comparison_type == "dni_vs_selfie":
            prompt = """
            Act√∫as como sistema de verificaci√≥n documental automatizado para entidad financiera.
            
            TAREA: Verificar calidad y autenticidad de im√°genes para proceso KYC.
            
            IMAGEN 1: Documento de identidad oficial
            IMAGEN 2: Foto de verificaci√≥n del cliente
            
            AN√ÅLISIS REQUERIDO:
            - Calidad t√©cnica de ambas im√°genes (nitidez, iluminaci√≥n, resoluci√≥n)
            - Detectar posibles manipulaciones digitales o alteraciones
            - Evaluar si son fotos genuinas (no pantallas, impresiones, etc.)
            - Verificar coherencia temporal y de edad entre im√°genes
            - Analizar caracter√≠sticas f√≠sicas visibles (color ojos, estructura facial)
            
            IMPORTANTE: Esto es an√°lisis t√©cnico automatizado para cumplimiento KYC/AML.
            
            Responde en JSON:
            {
                "face_match": true/false,
                "confidence": 0-100,
                "analysis": "an√°lisis t√©cnico de calidad y autenticidad",
                "fraud_indicators": ["se√±ales", "de", "manipulaci√≥n", "detectadas"],
                "recommendation": "APPROVE/REVIEW/REJECT"
            }
            """
        elif comparison_type == "selfie_vs_video":
            prompt = """
            Compara estas dos im√°genes:
            1. PRIMERA IMAGEN: Selfie de una persona
            2. SEGUNDA IMAGEN: Frame de video de verificaci√≥n
            
            ANALIZA:
            - ¬øSon la misma persona?
            - ¬øEl video muestra una persona REAL (no una foto/pantalla)?
            - ¬øSe ven movimientos naturales o signos de vida?
            - ¬øHay indicios de que es una foto mostrada en pantalla?
            
            BUSCA SIGNOS DE VIDA REAL:
            - Movimientos naturales de ojos/cara
            - Cambios de luz/sombra realistas
            - Profundidad en lugar de superficie plana
            - Calidad de imagen coherente con video real
            
            Responde en JSON:
            {
                "face_match": true/false,
                "is_live_person": true/false,
                "confidence": 0-100,
                "analysis": "explicaci√≥n detallada",
                "fraud_indicators": ["lista", "de", "problemas", "detectados"],
                "recommendation": "APPROVE/REVIEW/REJECT"
            }
            """
        else:
            prompt = """
            Compara estas dos im√°genes de caras y determina si son la misma persona.
            
            Analiza similitudes faciales y proporciona una evaluaci√≥n detallada.
            
            Responde en JSON:
            {
                "face_match": true/false,
                "confidence": 0-100,
                "analysis": "explicaci√≥n de tu an√°lisis",
                "recommendation": "APPROVE/REVIEW/REJECT"
            }
            """
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "Eres un especialista en verificaci√≥n de documentos de identidad y detecci√≥n de fraude. Tu trabajo es analizar documentos oficiales y fotos de verificaci√≥n para procesos KYC (Know Your Customer) en entidades financieras. Proporciona an√°lisis t√©cnicos precisos."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_image1}"}
                        },
                        {
                            "type": "image_url", 
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_image2}"}
                        }
                    ]
                }
            ],
            max_tokens=1200,
            temperature=0.1
        )
        
        result = response.choices[0].message.content
        print(f"ü§ñ GPT Vision resultado: {result[:100]}...")
        
        # Intentar parsear como JSON
        try:
            return json.loads(result)
        except:
            # Si no es JSON v√°lido, devolver formato b√°sico
            return {
                "face_match": False,
                "confidence": 0,
                "analysis": result,
                "recommendation": "REVIEW",
                "error": "Respuesta no fue JSON v√°lido"
            }
            
    except Exception as e:
        print(f"‚ùå Error en comparaci√≥n GPT Vision: {e}")
        return {
            "face_match": False,
            "confidence": 0,
            "analysis": f"Error: {str(e)}",
            "recommendation": "REJECT"
        }

def analyze_and_compare_dni_with_gpt(image_path, user_data):
    """
    GPT-4 Vision analiza DNI Y compara con datos del usuario
    Devuelve resultado completo de verificaci√≥n
    """
    try:
        # Convertir imagen a base64
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        # Prompt para an√°lisis completo
        prompt = f"""
        Analiza esta imagen de documento de identidad y compara con los datos del usuario.
        
        DATOS DEL USUARIO A VERIFICAR:
        - Nombre: {user_data['firstName']} {user_data['lastName']}
        - N√∫mero de documento: {user_data['documentNumber']}
        - Nacionalidad: {user_data['nationality']}
        - Fecha nacimiento: {user_data['birthDate']}
        - Fecha expedici√≥n: {user_data['issueDate']}
        - Fecha validez: {user_data['expiryDate']}
        
        TAREAS CR√çTICAS:
        1. IDENTIFICAR EL TIPO DE DOCUMENTO y verificar que corresponde al pa√≠s {user_data['nationality']}
        2. Extrae TODOS los datos visibles del documento
        3. Compara cada campo con los datos del usuario
        4. S√© tolerante con errores de OCR y variaciones
        5. VERIFICAR que el documento es del pa√≠s correcto (ej: DNI espa√±ol para ESP, CNI franc√©s para FRA, etc.)
        
        PA√çSES Y SUS DOCUMENTOS:
        - ESP: DNI espa√±ol
        - FRA: Carte Nationale d'Identit√© (CNI)
        - ITA: Carta d'Identit√†
        - PRT: Cart√£o de Cidad√£o
        - DEU: Personalausweis
        - USA: Driver's License / State ID
        - MEX: INE
        - ARG: DNI argentino
        - COL: C√©dula de Ciudadan√≠a
        
        Responde EXACTAMENTE en este formato JSON:
        {{
            "extracted_text": "texto completo extra√≠do del documento",
            "document_analysis": {{
                "document_type": "tipo de documento detectado",
                "country_match": true/false,
                "is_valid_document": true/false
            }},
            "verification": {{
                "name_match": true/false,
                "document_number_match": true/false,
                "birthdate_match": true/false,
                "issue_date_match": true/false,
                "expiry_date_match": true/false,
                "country_verification": true/false,
                "overall_confidence": 0-100,
                "recommendation": "APPROVE/REJECT/REVIEW"
            }},
            "extracted_data": {{
                "name": "nombre extra√≠do",
                "document_number": "n√∫mero extra√≠do", 
                "birthdate": "fecha nacimiento extra√≠da",
                "issue_date": "fecha expedici√≥n extra√≠da",
                "expiry_date": "fecha validez extra√≠da",
                "nationality": "nacionalidad detectada"
            }},
            "details": "explicaci√≥n breve de la verificaci√≥n y pa√≠s"
        }}
        """
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "Eres un experto en verificaci√≥n de documentos de identidad internacionales. Analiza con precisi√≥n y verifica que el documento corresponde al pa√≠s indicado. S√© tolerante con variaciones normales pero estricto con la correspondencia pa√≠s-documento."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=1200,
            temperature=0.1
        )
        
        result = response.choices[0].message.content
        print(f"üß† GPT-4 Smart Analysis: {result[:200]}...")
        
        # Limpiar respuesta: eliminar marcadores de c√≥digo markdown
        cleaned_result = result.strip()
        if cleaned_result.startswith('```json'):
            cleaned_result = cleaned_result[7:]  # Eliminar '```json'
        if cleaned_result.endswith('```'):
            cleaned_result = cleaned_result[:-3]  # Eliminar '```'
        cleaned_result = cleaned_result.strip()
        
        # Intentar parsear como JSON
        try:
            json_result = json.loads(cleaned_result)
            return json.dumps(json_result)
        except Exception as e:
            print(f"‚ö†Ô∏è Error parseando JSON: {e}")
            print(f"üîç Texto limpio: {cleaned_result[:100]}...")
            # Si no es JSON v√°lido, crear estructura b√°sica con el texto extra√≠do
            fallback_result = {
                "extracted_text": result,
                "verification": {
                    "name_match": False,
                    "dni_match": False,
                    "birthdate_match": False,
                    "address_match": False,
                    "overall_confidence": 0,
                    "recommendation": "REVIEW"
                },
                "extracted_data": {},
                "details": "GPT devolvi√≥ texto plano en lugar de JSON"
            }
            return json.dumps(fallback_result)
        
    except Exception as e:
        print(f"‚ùå GPT-4 Smart Analysis Error: {e}")
        # SIN FALLBACK - Solo GPT-4
        return json.dumps({
            "error": f"Error en an√°lisis GPT-4: {str(e)}",
            "extracted_text": "",
            "verification": {
                "name_match": False,
                "dni_match": False,
                "birthdate_match": False,
                "address_match": False,
                "overall_confidence": 0,
                "recommendation": "REJECT"
            },
            "extracted_data": {
                "name": "Error",
                "dni": "Error",
                "birthdate": "Error",
                "address": "Error"
            },
            "details": "No se pudo conectar con GPT-4 Vision"
        })


